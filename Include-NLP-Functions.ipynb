{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include-NLP-Functions\n",
    "Define NLP functions used in the other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim # used for the Mallet LDA model\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "#from gensim import corpora, models, similarities\n",
    "from gensim import corpora, models\n",
    "#from gensim.similarities import Similarity\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import LineSentence # use when reading sentences from large files\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# For pre-trained GloVe word2vec models\n",
    "## See https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# For pre-trained FastText word2vec models\n",
    "## To be implemented\n",
    "## See https://datascience.stackexchange.com/questions/20071/how-do-i-load-fasttext-pretrained-model-with-gensim\n",
    "#from gensim.models.wrappers import FastText\n",
    "#model = FastText.load_fasttext_format('wiki.simple')\n",
    "#print(model.most_similar('teacher'))\n",
    "# Output = [('headteacher', 0.8075869083404541), ('schoolteacher', 0.7955552339553833), ('teachers', 0.733420729637146), ('teaches', 0.6839243173599243), ('meacher', 0.6825737357139587), ('teach', 0.6285147070884705), ('taught', 0.6244685649871826), ('teaching', 0.6199781894683838), ('schoolmaster', 0.6037642955780029), ('lessons', 0.5812176465988159)]\n",
    "#print(model.similarity('teacher', 'teaches'))\n",
    "# Output = 0.683924396754\n",
    "\n",
    "\n",
    "# From https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple corpus-building functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a corpus from a list of strings\n",
    "# Output is a list containing lists of tokens\n",
    "\n",
    "def build_corpus_from_strings(listOfStrings, remove_words=[]):\n",
    "    corpus = []\n",
    "    for text in listOfStrings:\n",
    "        # Strip the string of punctuations\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # lowercase everything\n",
    "        text = text.lower()\n",
    "        doc = [token for token in text.split() if token not in remove_words]\n",
    "        corpus.append(doc)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the text occurring in text_field, returns a tokenized corpus\n",
    "# with a list of documents, each document itself a list of tokens.\n",
    "#### NOTE: NO CLEANING ####\n",
    "def build_corpus(dataFrame, text_field, remove_words=[]):\n",
    "    corpus = []\n",
    "    for text in dataFrame[text_field]:\n",
    "        doc = [token for token in text.split() if token not in remove_words]\n",
    "        corpus.append(doc)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given tokenized lists, build it into one list -- i.e., a corpus\n",
    "#### NOTE: The output will be a list of documents where each document is a list of tokens ####\n",
    "#### Flatten it as needed to build a single list of tokens\n",
    "def build_corpus_from_tokenized_lists(dataFrame, tokenized_text_field):\n",
    "    t0 = time.time()\n",
    "    corpus = list(itertools.chain(dataFrame[tokenized_text_field][0:]))\n",
    "    t1 = time.time()\n",
    "    print(\"Execution time = {} seconds.\".format(round(t1-t0, 1)))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best way to create a corpus from a list of text strings, each text string being a document in the corpus\n",
    "## Depends on the prep_doc function below\n",
    "# Create a corpus using documents as input and the prep_doc function below\n",
    "# A document is a text string, e.g. 'The fox jumped over the hen.'\n",
    "def build_prepped_corpus(dataFrame, text_field):\n",
    "    corpus = []\n",
    "    doc = [prep_doc(text_string) for text_string in dataFrame[text_field]]\n",
    "    corpus.append(doc)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LATEST - USE THIS ####\n",
    "# Build a text corpus (a list of tokenized lists of words) given \n",
    "## a dataframe and a text field in that dataframe\n",
    "def build_text_corpus(dataFrame, text_field):\n",
    "    t0 = time.time()\n",
    "    # Get the text field as a list of documents\n",
    "    docs = list(dataFrame[text_field])\n",
    "    t1 = time.time()\n",
    "    print(\"Documents pulled from dataframe in {} seconds.\".format(round(t1-t0, 2)))\n",
    "    \n",
    "    # Clean each document as needed\n",
    "    t2 = time.time()\n",
    "    print(\"Starting to clean documents...this can take a while....patience...\")\n",
    "    text_corpus = [clean_doc(doc) for doc in docs]\n",
    "    t3 = time.time()\n",
    "    print(\"Documents tokenized and cleaned in {} seconds.\".format(round(t3-t2), 2))\n",
    "    \n",
    "    return text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the descriptive stats on number of words in each document in the corpus\n",
    "def get_corpus_stats(text_corpus):\n",
    "    # text_corpus is a list of documnets; each document is a tokenized list\n",
    "    # Flatten text_corpus to get one single list\n",
    "    t0 = time.time()\n",
    "    num_docs = len(text_corpus)\n",
    "    text_corpus_flattened = flatten_list(text_corpus)\n",
    "    total_words = len(text_corpus_flattened)\n",
    "    doc_lengths = [len(doc) for doc in text_corpus]\n",
    "    mean_word_count = round(np.mean(doc_lengths), 0)\n",
    "    std_dev_word_count = round(np.std(doc_lengths), 0)\n",
    "    max_word_count = np.max(doc_lengths)\n",
    "    min_word_count = np.min(doc_lengths)\n",
    "    \n",
    "    print(\"Number of documents in the corpus = {}\".format(num_docs))\n",
    "    print(\"Total number of words in the corpus = {}.\".format(total_words))\n",
    "    print(\"Average number of words per document = {}.\".format(mean_word_count))\n",
    "    print(\"Std Dev of words per document = {}.\".format(std_dev_word_count))\n",
    "    print(\"Largest document has {} words.\".format(max_word_count))\n",
    "    print(\"Smallest document has {} words.\".format(min_word_count))\n",
    "    t1 = time.time()\n",
    "    print(\"Execution time = {} seconds.\".format(round(t1-t0, 3)))\n",
    "    print(\"[Mean Word Count, Std Word Count, Max Word Count, Min Word Count]\")\n",
    "    return [mean_word_count, std_dev_word_count, max_word_count, min_word_count]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document cleaning functions\n",
    "\n",
    "Jason Brownlee has a good [tutorial](https://machinelearningmastery.com/clean-text-machine-learning-python/) on cleaning text using plain Python or using NLTK.\n",
    "\n",
    " - Tokenize the document\n",
    " - Convert the tokens to lower case\n",
    " - Remove punctuation and clean up empty strings\n",
    " - Remove hex sequences and clean up empty strings\n",
    " - Optional cleaning\n",
    "  - Remove standard English stop words\n",
    "  - Remove additional words (given a list of these words)\n",
    "  - Remove numbers\n",
    "  - Remove additional characters (given a regex)\n",
    "  - Lemmatize the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a string and remove all whitespaces\n",
    "def tokenize_text(text):\n",
    "    # text is any string, e.g., 'The brown fox jumped over the quick hen! And then what?'\n",
    "    # It can have any characters including punctuations, hex sequences, numerals, etc.\n",
    "    \n",
    "    # Use the NLTK tokenizer \n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "    except TypeError:\n",
    "        #print(\"TypeError\")\n",
    "        tokens = []\n",
    "    except AttributeError:\n",
    "        #print(\"AttributeError\")\n",
    "        tokens = []\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty strings from a tokenized list of strings (utility function)\n",
    "def remove_empty_tokens(doc):\n",
    "    # doc is any list of tokenized strings, e.g., the output of tokenize_text\n",
    "    # Empty strings have truth value FALSE; hence non-empty strings are TRUE\n",
    "    ## https://stackoverflow.com/questions/9573244/most-elegant-way-to-check-if-the-string-is-empty-in-python\n",
    "    return [token for token in doc if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tokens into lower case\n",
    "def lower_case(doc):\n",
    "    # doc is any tokenized list of strings\n",
    "    return [token.lower() for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEPRECATED ####\n",
    "# Remove punctuation\n",
    "def remove_punctuation(doc):\n",
    "    # doc is any tokenized list of strings\n",
    "    \n",
    "    # Translation table for removing punctuations (can also be done using regex)\n",
    "    ## https://stackoverflow.com/questions/34293875\n",
    "    #translator = str.maketrans('', '', string.punctuation)\n",
    "    #tokens = [token.translate(translator) for token in doc]\n",
    "    \n",
    "    # Use regex instead (see remove_punct)\n",
    "    tokens = [re.sub(r'['+string.punctuation+']', r'', token) for token in doc]\n",
    "    \n",
    "    # Remove the empty tokens after punctuation has been removed\n",
    "    return remove_empty_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A better way to remove punctuation\n",
    "def remove_punct(doc, extent='full'):\n",
    "    # doc is a tokenized list of strings\n",
    "    # extent = 'full' -- use the full punctuation list\n",
    "    # extent = 'select' -- use select punct_list below\n",
    "    \n",
    "    # string.punctuation consists of '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    # tailor to a more specific list of characters\n",
    "    punct_list = '!\"()\\',.:;?<>[]$`{}'\n",
    "    \n",
    "    if extent == 'full': \n",
    "        # Use to remove the full list of punctuation characters\n",
    "        tokens = [''.join(c for c in token if c not in string.punctuation) for token in doc]\n",
    "    elif extent == 'select':\n",
    "        # Use to remove a subset of punctuation characters\n",
    "        tokens = [''.join(c for c in token if c not in punct_list) for token in doc]\n",
    "    else:\n",
    "        # default to the full list\n",
    "        # Use to remove the full list of punctuation characters\n",
    "        tokens = [''.join(c for c in token if c not in string.punctuation) for token in doc]\n",
    "    \n",
    "    # remove the empty tokens\n",
    "    return remove_empty_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hex sequences\n",
    "def remove_hex(doc): \n",
    "    # doc is a tokenized list of strings\n",
    "    tokens = [re.sub(r'[^\\x00-\\x7F]', r'', token) for token in doc]\n",
    "    # Remove the empty tokens after hex sequences have been removed\n",
    "    return remove_empty_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "def remove_stopwords(doc):\n",
    "    # doc is any tokenized list of strings \n",
    "    ## e.g., ['the', 'little', 'brown', 'fox', 'jumped', 'over']\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in doc if not token in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove additional words\n",
    "def remove_words(doc, word_list=[]):\n",
    "    # doc is any tokenized list of strings\n",
    "    # word_list is the list of words to be removed from the doc \n",
    "    return [token for token in doc if not token in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove sequences of numerals that occur as separate tokens\n",
    "## This will remove '1234' but not 'dec1234'\n",
    "def remove_numbers(doc):\n",
    "    # doc is any list of tokenized strings\n",
    "    tokens = [re.sub(r'\\b\\d+\\b', r'', token) for token in doc]\n",
    "    # Remove the empty tokens after standalone numerical sequences have been removed\n",
    "    return remove_empty_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO DO - DOESN'T WORK YET ####\n",
    "# Not sure how to pass the raw regex string pattern to the function\n",
    "# Remove a select sequence of characters\n",
    "def remove_custom_token(doc, regex_pattern):\n",
    "    # doc is any list of tokenized strings\n",
    "    # regex_pattern is a token pattern to search for, e.g., '\\b\\d+\\b' or '[^\\x00-\\x7F]'\n",
    "    ## this string has to be converted into a raw string\n",
    "    ## don't know how to do this yet...\n",
    "    tokens = [re.sub(regex_pattern, r'', token) for token in doc]\n",
    "    # Remove the empty tokens after standalone numerical sequences have been removed\n",
    "    return remove_empty_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the tokens\n",
    "def lemmatize(doc):\n",
    "    # doc is any list of tokenized strings\n",
    "    return [wordnet_lemmatizer.lemmatize(token) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the functions above in sequence to tokenize and clean a document\n",
    "def clean_doc(text, rem_punct=1, rem_hex=1, rem_nums=1, rem_stopwords=1):\n",
    "    # text is a string of text\n",
    "    # Clean each document as needed\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = lower_case(tokens)\n",
    "    \n",
    "    if rem_punct == 1:\n",
    "        tokens = remove_punct(tokens, extent='full')\n",
    "    if rem_hex == 1:\n",
    "        tokens = remove_hex(tokens)\n",
    "    if rem_nums == 1:\n",
    "        tokens = remove_numbers(tokens)\n",
    "    if rem_stopwords == 1: \n",
    "        tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for building n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build n-grams models by training the Gensim phraser on the corpus\n",
    "## 2-grams model is built by providing the 1-gram corpus as input\n",
    "## 3-grams model is built by providing the 2-gram corpus as input\n",
    "## and so on...\n",
    "\n",
    "def build_ngram_model(text_corpus_ngram, ngram_size, file_path):\n",
    "    # text_corpus_ngram is list of documents where each document is a list of tokens\n",
    "    # ngram_size is the n of the n-gram - used to give the output the right name\n",
    "    # file_path = intermediate_dir_path\n",
    "    \n",
    "    # Train a model to recognize n-word phrases on the corpus\n",
    "    t0 = time.time()\n",
    "    model_ngram = Phrases(text_corpus_ngram, min_count=1, threshold=1)\n",
    "    t1 = time.time()\n",
    "    print(str(ngram_size) + \"-gram phraser model trained in {} seconds.\".format(round(t1-t0, 1)))\n",
    "    \n",
    "    # Create a more efficient model for future use\n",
    "    model_ngram_fast = Phraser(model_ngram)\n",
    "    \n",
    "    # Save the phraser model_ngram_fast for future use\n",
    "    path_to_file = os.path.join(file_path, 'phraser_ngram_' + str(ngram_size))\n",
    "    model_ngram_fast.save(path_to_file)\n",
    "    t2 = time.time()\n",
    "    \n",
    "    print(\"Efficient N-gram phraser model created and saved to {} in {} seconds.\".format(path_to_file, round(t2-t1, 1)))\n",
    "    print(\"Load the efficient phraser model using models.phrases.Phraser.load(\" + path_to_file +\")\")\n",
    "    \n",
    "    return model_ngram_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For any document in the text corpus, apply an n-gram model \n",
    "## to get the n-gram version of that document\n",
    "def apply_ngram_model(n_gram_model, document, ngram_size):\n",
    "    t0 = time.time()\n",
    "    n_gram_doc = n_gram_model[document]\n",
    "    t1 = time.time()\n",
    "    #print(\"Created {}-gram document in {} seconds.\".format(ngram_size, round(t1-t0, 1)))\n",
    "    return n_gram_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and apply the Gensim n-gram model to the text_corpus\n",
    "## 2-grams model is built by providing the 1-gram text corpus as input\n",
    "## 3-grams model is built by providing the 2-gram text corpus as input\n",
    "## and so on...\n",
    "def gensim_n_gram_corpus(text_corpus_n_gram, n_gram_size, file_path, file_name):\n",
    "    # Create the model\n",
    "    model_ngram_fast = build_ngram_model(text_corpus_n_gram, n_gram_size, file_path)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    # Use the model to generate n-grams for each document in the corpus\n",
    "    text_corpus_n_grams = [apply_ngram_model(model_ngram_fast, doc, n_gram_size) \n",
    "                           for doc in text_corpus_n_gram]\n",
    "    t1 = time.time()\n",
    "    print(\"Created Gensim \" + str(n_gram_size) + \"-gram corpus in {} seconds.\".format(round(t1-t0, 2)))\n",
    "    \n",
    "    # Save the n-gram corpus\n",
    "    # Pickle the text corpus for next step\n",
    "    pickle_list(os.path.join(file_path, file_name), text_corpus_n_grams)\n",
    "    print(\"Saved the \" + str(n_gram_size) + \"-gram text corpus to {}\".format(os.path.join(file_path, file_name)))\n",
    "    \n",
    "    return text_corpus_n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given corpus, create the Gensim dictionary\n",
    "def create_dictionary(corpus_text, ngram_size, save_location):\n",
    "    # corpus_text is an n-gram corpus for n >= 1\n",
    "    # ngram_size is the size of the ngrams in the corpus\n",
    "    # save_location is, e.g., os.path.join(intermediate_dir_path, 'dictionary_ngram_size.dict')\n",
    "    \n",
    "    #### Dictionary filtering parameters ####\n",
    "    # Words that appear in less than or equal to NO_BELOW documents in the corpus\n",
    "    NO_BELOW = 1\n",
    "    # Words that appear in more than NO_ABOVE percentage of the documents in the corpus\n",
    "    NO_ABOVE = 0.99\n",
    "    \n",
    "    t0 = time.time()\n",
    "    print(\"Starting to create dictionary...\")\n",
    "    dic = Dictionary(corpus_text)\n",
    "    t1 = time.time()\n",
    "    print(\"N-gram size \" + str(ngram_size) + \" Dictionary created in {} seconds...\".format(round(t1-t0, 1)))\n",
    "    print(\"The N-gram size {} dictionary is {:,} words long.\".format(ngram_size, len(dic)))\n",
    "    \n",
    "    # Remove words that are very rare or too common from the dictionary\n",
    "    dic.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE)\n",
    "    print(\"The filtered N-gram size {} dictionary is {:,} words long.\".format(ngram_size, len(dic)))\n",
    "    \n",
    "    # Reassign integer ids (compactify) the filtered dictionary\n",
    "    t2 = time.time()\n",
    "    dic.compactify()\n",
    "    # Save the dictionary for future use\n",
    "    dic.save(save_location)\n",
    "    t3 = time.time()\n",
    "    print(\"N-gram size \" + str(ngram_size) + \" Dictionary compacted and saved in {} seconds...\".format(round(t3-t2, 1)))\n",
    "    \n",
    "    # Load the dictionary using\n",
    "    ## dictionary = corpora.Dictionary.load(path_to_dictionary_file)\n",
    "    print(\"Load the dictionary using corpora.Dictionary.load(path_to_file)\")\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given corpus, use the dictionary to create a bag of words\n",
    "def create_bag_of_words(corpus_text, dictionary, ngram_size, save_location):\n",
    "    # corpus_text is an n-gram corpus for n >= 1\n",
    "    # dictionary is the matching n-gram dictionary\n",
    "    # ngram_size is the size of the ngrams in the corpus\n",
    "    # save_location is os.path.join(intermediate_dir_path, 'bow_ngram_size.mm')\n",
    "    \n",
    "    print(\"Starting to create the Bag of Words...\")\n",
    "    t0 = time.time()\n",
    "    bow = [dictionary.doc2bow(item) for item in corpus_text]\n",
    "    t1 = time.time()\n",
    "    print(\"N-grams size {} bag of words created in {} seconds.\".format(ngram_size, round(t1-t0, 1)))\n",
    "    \n",
    "    # Save the BoW corpus for later use\n",
    "    t2 = time.time()\n",
    "    corpora.MmCorpus.serialize(save_location, bow)\n",
    "    t3 = time.time()\n",
    "    print(\"N-grams size {} bag of words saved in {} seconds.\".format(ngram_size, round(t3-t2, 1)))\n",
    "    print(\"Load the BoW corpus using - corpora.MmCorpus(full_path_to_BoW_corpus_name)\")\n",
    "\n",
    "    # Load the BoW corpus using\n",
    "    ## corpus_name = corpora.MmCorpus(full_path_to_corpus_name)\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram generation\n",
    "## Another way to do n-grams without relying on Gensim's Phraser\n",
    "# Generates a list of strings, each string consisting of n words\n",
    "# From https://programminghistorian.org/lessons/keywords-in-context-using-n-grams#from-text-to-n-grams\n",
    "def getNGrams(wordlist, n, join_charac = '_'):\n",
    "    # wordlist is the output of prep_doc or any list of strings\n",
    "    # it must be a tokenized list, e.g., ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'hen']\n",
    "    ngrams = []\n",
    "    for i in range(len(wordlist)-(n-1)):\n",
    "        ngrams.append(wordlist[i:i+n])\n",
    "        \n",
    "    # Join the words in each list\n",
    "    ngrams_joined = [join_charac.join(list) for list in ngrams]\n",
    "    \n",
    "    return ngrams_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram generation -- NLTK style\n",
    "# Modified from https://programminghistorian.org/lessons/keywords-in-context-using-n-grams#from-text-to-n-grams\n",
    "# Generates a list of strings, each string consisting of 1, 2, ..., n words\n",
    "# This is the way NLTK generates n-grams\n",
    "def getNGramsFull(wordlist, n, join_charac = '_'):\n",
    "    # wordlist is the output of prep_doc or any list of strings\n",
    "    # it must be a tokenized list, e.g., ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'hen']\n",
    "    ngrams = []\n",
    "    for i in range(len(wordlist)-(n-1)):\n",
    "        for j in range(n):\n",
    "            ngrams.append(wordlist[i:i+j+1])\n",
    "        \n",
    "    # Join the words in each list\n",
    "    ngrams_joined = [join_charac.join(list) for list in ngrams]\n",
    "    \n",
    "    return ngrams_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from From http://www.albertauyeung.com/post/generating-ngrams-python/\n",
    "def generate_ngrams(doc, n, join_charac = '_'):\n",
    "    # doc is a document in a text corpus\n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[doc[i:] for i in range(n)])\n",
    "    return [join_charac.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple n-grams and pickle the list for later use\n",
    "def gen_pkl_ngrams(text_corpus, size, pkl_file_path_plus_name):\n",
    "    text_corpus_n_grams = [generate_ngrams(doc, size) for doc in text_corpus]\n",
    "    # Pickle the list for later use\n",
    "    # Pickle the text corpus for next step\n",
    "    pickle_list(pkl_file_path_plus_name, text_corpus_n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating n-grams at the character level\n",
    "## Modified from https://bergvca.github.io/2017/10/14/super-fast-string-matching.html\n",
    "def char_ngrams(string, size=3):\n",
    "    #string = re.sub(r'[,-./]|',r'', string)\n",
    "    string = \" \".join(clean_doc(string))\n",
    "    ngrams = zip(*[string[i:] for i in range(size)])\n",
    "    return [''.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEPRECATED ####\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# import string\n",
    "\n",
    "# # Translation table for removing punctuations\n",
    "# table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# #### Stop words ####\n",
    "# STOP_WORDS_0 = []\n",
    "# #STOP_WORDS_1 = set(stopwords.words('english'))\n",
    "# #### SET STOP_WORDS HERE ####\n",
    "# STOP_WORDS = STOP_WORDS_0\n",
    "\n",
    "# # Remove specific words that occur in the corpus\n",
    "# REMOVE_WORDS_0 = [] # Use when you don't need to remove anything specific\n",
    "# # Identified after an initial analysis of the corpus\n",
    "# REMOVE_WORDS_1 = ['bc', 'at', 'asking', 'we', 'hoping', 'meeting', 'understand', 'inquiry', \n",
    "#                 'could', 'need', 'request', 'looking', 'v', 'u', 'etc', 'client', 'would', \n",
    "#                 'you', 'like', 'speak', 'schedule', 'call', 'analyst', 'discus', 'me', 'hi', \n",
    "#                 'hello', 'follow', 'up', 'set', 'question', 'thought', 'please', 'thank',\n",
    "#                ]\n",
    "\n",
    "# #### SET REMOVE_WORDS HERE ####\n",
    "# REMOVE_WORDS = REMOVE_WORDS_0\n",
    "\n",
    "# #### SET LEMMATIZE OFF/ON HERE ####\n",
    "# LEMMATIZE = 1 # Set to 1 to turn on\n",
    "\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def prep_doc(document, stop_words=STOP_WORDS_0, remove_words=REMOVE_WORDS_0, lemmatize=0):\n",
    "#     '''\n",
    "#     Following https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "    \n",
    "#     1. Tokenize the entire document on whitespace.\n",
    "#     2. Remove punctuation.\n",
    "#     3. Normalize case.\n",
    "#     4. Remove stopwords.\n",
    "#     5. Lemmatize\n",
    "#     6. Clean up the remaining items -- non-ASCII characters, empty strings, specific words, numbers\n",
    "#     '''\n",
    "#     # Function defaults are set conservatively - just remove English stop words\n",
    "    \n",
    "#     # Tokenize\n",
    "#     #tokens = nltk.word_tokenize(document)\n",
    "#     # Handle NULL documents\n",
    "#     try:\n",
    "#         tokens = document.split()\n",
    "#     except AttributeError:\n",
    "#         return ['EMPTY']\n",
    "    \n",
    "#     # Strip all punctuations\n",
    "#     stripped = [token.translate(table) for token in tokens]\n",
    "    \n",
    "#     # Normalize case\n",
    "#     normalized = [strip.lower() for strip in stripped]\n",
    "    \n",
    "#     # Remove stopwords\n",
    "#     stopped = [norm for norm in normalized if not norm in stop_words]\n",
    "    \n",
    "#     # Lemmatize\n",
    "#     if lemmatize == 1:\n",
    "#         lemmatized = [wordnet_lemmatizer.lemmatize(stop) for stop in stopped]\n",
    "#     else:\n",
    "#         lemmatized = stopped\n",
    "    \n",
    "#     # Remove non-ASCII tokens (e.g., '\\x96')\n",
    "#     asciied = [re.sub(r'[^\\x00-\\x7F]', r'', lem) for lem in lemmatized]\n",
    "    \n",
    "#     # Remove empty tokens ''\n",
    "#     # Empty strings have truth value FALSE; hence non-empty strings are TRUE\n",
    "#     # https://stackoverflow.com/questions/9573244/most-elegant-way-to-check-if-the-string-is-empty-in-python\n",
    "#     misc = [asc for asc in asciied if asc]\n",
    "    \n",
    "#     # remove strings that are numerals\n",
    "#     cleaned = [mis for mis in misc if mis.isdigit() == False]\n",
    "    \n",
    "#     final = [clean for clean in cleaned if not clean in remove_words]\n",
    "    \n",
    "#     return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple text functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency counts of words in a corpus from the top of the list to a specific point\n",
    "## or from any specific point in the list all the way to the bottom of the list\n",
    "# Calculate the frequency of occurrence of words\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "def word_freq(token_list, num_results=30, list_type='corpus'):\n",
    "    # token_list can be a corpus (e.g., the output of build_corpus), or\n",
    "    ## it can be a simple list of tokens.\n",
    "    # list_type = 'corpus' (default) or 'simple list' (this is the type of list in token_list)\n",
    "    \n",
    "    # If token_list is a corpus, \n",
    "    ## flatten the lists in corpus_text into one big list of words in the entire corpus\n",
    "    if list_type == 'corpus':\n",
    "        word_list = flatten_list(token_list)\n",
    "    elif list_type == 'simple list':\n",
    "        word_list = token_list\n",
    "    else:\n",
    "        return 'Sorry, the list has to be a simple list or a corpus. Try again.'\n",
    "        \n",
    "    frequency = defaultdict(int)\n",
    "    for word in word_list:\n",
    "        frequency[word] += 1\n",
    "    \n",
    "    if num_results > 0:\n",
    "        return sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)[0:num_results]\n",
    "    elif num_results < 0:\n",
    "        return sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)[num_results:]\n",
    "    else:\n",
    "        # return the frequencies for the entire vocabulary when num_results is set to 0\n",
    "        return sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return words that occur between 2 frequency values\n",
    "def word_freq_between(token_list, greater_than=200, less_than=500, list_type='corpus'):\n",
    "    \n",
    "    # Get the entire vocabulary by frequency\n",
    "    item_freq = word_freq(token_list, 0, list_type)\n",
    "    \n",
    "    return [(word[0], word[1]) for word in item_freq if (list(word)[1] >= greater_than) and (list(word)[1] < less_than)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search functions that don't require any NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very simple search function\n",
    "## Given a document and a word in that document, find all occurrences of the word\n",
    "## Then return the word along with its n neighbors on either side of it\n",
    "\n",
    "# If the corpus is small, then then it can be turned into a single document -- i.e.,\n",
    "## a single list of words.\n",
    "\n",
    "def get_word_context(document, word, context_type='full', context_window=3, include_word=False):\n",
    "    # find the indices of each occurrence of the word in the document\n",
    "    # document is a list of tokens, e.g., ['The', 'grey', 'fox', 'jumped', 'over', 'the', 'blue', hen']\n",
    "    # expression is the expression we're interested in; we'd like to know \n",
    "    ## the context in which it appears in the corpus\n",
    "    # context_type can be 'full' (default) for words surrounding the given word, \n",
    "    ## 'left' for words to the left of the given word, or 'right' for words to the right of the given word\n",
    "    # context_widow is the number of tokens to the left and right of the given word that are pulled\n",
    "    \n",
    "    doc_length = len(document)\n",
    "    \n",
    "    # Find the position of the word in the document \n",
    "    # It is returned as a list with a single element -- e.g., [4]\n",
    "    index = [i for i, x in enumerate(document) if x == word]\n",
    "    \n",
    "    context_full = []\n",
    "    context_left = [] # words to the left of the given word\n",
    "    context_right = [] # words to the right of the given word\n",
    "    \n",
    "    # get the word in context\n",
    "    # make sure the context window makes sense given the length of the document \n",
    "    ## and the position of the word in question\n",
    "    for ind in index:\n",
    "        if ind - context_window < 0:\n",
    "            left_index = 0\n",
    "        else:\n",
    "            left_index = ind - context_window\n",
    "        \n",
    "        if ind + context_window + 1 > doc_length:\n",
    "            right_index = doc_length\n",
    "        else:\n",
    "            right_index = ind + context_window + 1\n",
    "        \n",
    "        c_full = document[left_index:right_index]\n",
    "        c_left = document[left_index:ind]\n",
    "        c_right = document[ind + 1:right_index]\n",
    "        \n",
    "        if include_word:\n",
    "            context_full.append(c_full)\n",
    "        else:\n",
    "            context_full.append([x for x in c_full if x != word])\n",
    "            \n",
    "        context_left.append(c_left)\n",
    "        context_right.append(c_right)\n",
    "        \n",
    "    if context_type == 'full':\n",
    "        #print(\"Number of matches = {}\".format(len(context_full)))\n",
    "        return context_full\n",
    "    elif context_type == 'left':\n",
    "        #print(\"Number of matches = {}\".format(len(context_left)))\n",
    "        return context_left\n",
    "    elif context_type == 'right':\n",
    "        #print(\"Number of matches = {}\".format(len(context_right)))\n",
    "        return context_right\n",
    "    else:\n",
    "        return context_full # default is all of the words, both to the left and right of the given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the context for each document in the corpus for a given word\n",
    "def get_word_context_corpus(corpus, word, context_type='full', context_window=3, include_word=True):\n",
    "    \n",
    "    # corpus is a list of of documents -- i.e., a list of lists\n",
    "    \n",
    "    # Get the raw context\n",
    "    context_array = [get_word_context(item, word, context_type, context_window, include_word) for item in corpus]\n",
    "    \n",
    "    # Add an index number to each result\n",
    "    context_array = list(zip(range(len(corpus)), context_array))\n",
    "    \n",
    "    # Clean up the context\n",
    "    ## Remove all empty lists\n",
    "    context_array = [x for x in context_array if isListEmpty(x[1]) == False]\n",
    "    \n",
    "    num_notes_matched = len(context_array)\n",
    "    \n",
    "    match_lengths=[]\n",
    "    for array in context_array:\n",
    "        array_len = len(array)\n",
    "        match_lengths.append(array_len)\n",
    "    \n",
    "    num_total_matches = np.sum(match_lengths)\n",
    "    \n",
    "    # Return the context as a list of lists\n",
    "    #return context_comb_again\n",
    "    print(\"Number of call note matches = {}\".format(num_notes_matched))\n",
    "    print(\"Total number of matches = {}\".format(num_total_matches))\n",
    "    return pd.DataFrame(context_array, columns=['Note Number', 'Context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very simple search function to get word context across the entire corpus\n",
    "# This is for a corpus that's built out of individual files\n",
    "def get_word_context_fileList(corpus, file_list, word, window=6):\n",
    "    \n",
    "    file_info = []\n",
    "    doc_info = []\n",
    "    \n",
    "    doc_index = 0 # keeps track of the file associated with the document\n",
    "    for document in corpus:\n",
    "        doc_name = file_list[doc_index]\n",
    "        context = get_word_context(document, word, window)\n",
    "        # if the context is not an empty list, append it to doc_info\n",
    "        if context:\n",
    "            file_info.append(doc_name)\n",
    "            context_strings = []\n",
    "            for item in context:\n",
    "                context_strings.append(\" \".join(item))\n",
    "            doc_info.append(context_strings)\n",
    "        doc_index += 1\n",
    "        \n",
    "    # Put it into a dataframe for display\n",
    "    df_results = pd.DataFrame({'Call Note': file_info, 'Matching Phrases': doc_info})\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the rows of the dataframe that match the given key word or phrase \n",
    "## which can be a unigram, bigram, or trigram\n",
    "def get_keyword_matches(dataFrameFull, field_name, key_word):\n",
    "    # dataFrameFull is a dataframe that contains the unigram, bigram, and trigram tokens as columns -- \n",
    "    ## e.g., df_data_full\n",
    "    # field_name is the name of the column for particular kind of phrase that must be matched, \n",
    "    ## e.g., 'MCP Bigram Names' in df_data_full\n",
    "    \n",
    "    rows = []\n",
    "    t0 = time.time()\n",
    "    for i in range(len(dataFrameFull)):\n",
    "        if key_word in dataFrameFull[field_name][i]:\n",
    "            rows.append(dataFrameFull.iloc[i])\n",
    "            \n",
    "    t1 = time.time()\n",
    "    print(\"Found {} matches in {} seconds\".format(len(rows),round(t1-t0)))\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_match(text_to_check, search_phrase):\n",
    "    # Checks if all the tokens of search_phrase occur in text_to_check\n",
    "    # text_to_check is a clean tokenized list -- e.g., a document in a cleaned corpus\n",
    "    # only makes sense to use unigram corpora\n",
    "    # search_phrase can be any string, e.g., 'the brown Fox, jumped Over, the hen'\n",
    "    # Both text_to_check and search_phrase are considered bags of words -- \n",
    "    ## the order of the words don't matter. \n",
    "    ## This approach works well when documents lengths are small. \n",
    "    \n",
    "    # clean and tokenize the search_phrase\n",
    "    search_phrase_clean = clean_doc(search_phrase)\n",
    "    \n",
    "    # Check the length of the intersection between text_to_check and search_phrase_clean\n",
    "    intersection_cardinality = len(set.intersection(*[set(text_to_check), set(search_phrase_clean)]))\n",
    "    if intersection_cardinality == len(search_phrase_clean):\n",
    "        return text_to_check\n",
    "    else:\n",
    "        return \"NO MATCH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the rows of the dataframe that match the given key word or phrase\n",
    "## Only makes sense to use the unigram corpus \n",
    "def get_corpus_phrase_matches(corpus, search_phrase):\n",
    "    # corpus is a cleaned unigram text corpus\n",
    "    # search phrase can be any string, e.g., 'the brown Fox, jumped Over, the hen'\n",
    "    t0 = time.time()\n",
    "    # Use bag_of_words_match on each document in the corpus\n",
    "    match_array = [bag_of_words_match(item, search_phrase) for item in corpus]\n",
    "    \n",
    "    # Add an index number to each result\n",
    "    match_results = list(zip(range(len(corpus)), match_array))\n",
    "    \n",
    "    # Remove all \"NO MATCH\" items\n",
    "    matches = [x for x in match_results if x[1] != \"NO MATCH\"]\n",
    "            \n",
    "    t1 = time.time()\n",
    "    print(\"Phrase matched {} times in {} seconds\".format(len(matches), round(t1-t0, 2)))\n",
    "    return pd.DataFrame(matches, columns=['Note Number', 'Content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Load Search Indices for TF-IDF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DISABLE FOR NOW - GENSIM INSTALL ISSUE ####\n",
    "# Index a bow corpus using a TFIDF model\n",
    "def create_search_index(model, index_path_and_name, bow, dictionary):\n",
    "    t0 = time.time()\n",
    "    # transform corpus to tfidf space and index it\n",
    "    index = similarities.Similarity(index_path_and_name, \n",
    "                                    model[bow], \n",
    "                                    num_features=len(dictionary)\n",
    "                                   )\n",
    "    t1 = time.time()\n",
    "    print(\"Index created in {} seconds.\".format(round(t1-t0, 2)))\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DISABLE FOR NOW - GENSIM INSTALL ISSUE ####\n",
    "def load_search_index(index_path_and_name):\n",
    "    t0 = time.time()\n",
    "    index = similarities.Similarity.load(index_path_and_name)\n",
    "    t1 = time.time()\n",
    "    print(\"Index loaded in {} seconds.\".format(round(t1-t0, 2)))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search functions on pre-built NLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_input_string(input_string, phraser):\n",
    "    # input_string is any input of the form \"this is  a string\"\n",
    "    clean_string = clean_doc(input_string)\n",
    "    phrased_string = phraser[clean_string]\n",
    "    return phrased_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the results of a query\n",
    "def get_query_results(query_string, \n",
    "                      search_dict, \n",
    "                      df_corpus, \n",
    "                      df_cols_to_display, \n",
    "                      num_results=25\n",
    "                     ):\n",
    "    \n",
    "    '''\n",
    "    query_string is a string of any length\n",
    "    search_dict is a dict that contains the name of the phraser, dictionary,\n",
    "      model, and index to use for the search.\n",
    "    df_corpus is the complete dataframe of the corpus being searched  \n",
    "    df_cols_to_display are the cols of df_corpus to display in the search results dataframe\n",
    "    \n",
    "    '''\n",
    "    t0 = time.time()\n",
    "    # Process the query string into a list of tokens\n",
    "    #clean_query = prep_doc(query_string)\n",
    "    clean_query = clean_doc(query_string)\n",
    "    \n",
    "    # Convert the list of tokens into phrases if necessary\n",
    "    if search_dict['phraser'] != '':\n",
    "        phrased_query = search_dict['phraser'][clean_query]\n",
    "    else:\n",
    "        phrased_query = clean_query\n",
    "    \n",
    "    # For everything EXCEPT Doc2Vec proceed as follows\n",
    "    if search_dict['index'] != 'doc2vec':\n",
    "        # Use the dictionary to transform the phrased_query into a bag of words vector\n",
    "        bow_query = search_dict['dictionary'].doc2bow(phrased_query)\n",
    "    \n",
    "        # Transform the bag of words vector into a vector in the topic model's space\n",
    "        model_query = search_dict['model'][bow_query]\n",
    "    \n",
    "        # Calculate the similarity of the query to each document in the corpus\n",
    "        sims = search_dict['index'][model_query]\n",
    "    \n",
    "        # Sort the similarity scores in descending order\n",
    "        sims_sorted = sorted(enumerate(sims), key=lambda item: -item[1])[0:num_results]\n",
    "    else:\n",
    "        # Create the Doc2Vec sims on the fly\n",
    "        query_vector = search_dict['model'].infer_vector(phrased_query)\n",
    "        sims_sorted = d2v_trigram_100.docvecs.most_similar(positive=[query_vector], topn=num_results)\n",
    "    \n",
    "    # Build a dataframe for displaying the search results\n",
    "    dataFrame_content = []\n",
    "    for item in sims_sorted:\n",
    "        dataFrame_content.append(df_corpus.iloc[item[0]][df_cols_to_display].values)\n",
    "        \n",
    "    df_results = pd.DataFrame.from_records(dataFrame_content, columns=df_cols_to_display)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(\"Search results obtained in {:.3} seconds.\".format(t1-t0))\n",
    "    print(\"The query is: {}\".format(query_string))\n",
    "    print(\"Here are the top {} results...\".format(num_results))\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity is (among other things) a metric for measuring how well a model matches an observation to an \n",
    "# existing observation.\n",
    "## The essence of Jaccard Similarity is to find, for any two sets, the number of elements in the intersection divided by the number of elements \n",
    "## in the union of the sets.\n",
    "# Slightly modified from \n",
    "#    http://dataconomy.com/2015/04/implementing-the-five-most-popular-similarity-measures-in-python/\n",
    "def jaccard_similarity(x,y):\n",
    "    # x and y are tokenized sentences\n",
    "    #print(set(x))\n",
    "    #print(set(y))\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    \n",
    "    try:\n",
    "        jac_score = intersection_cardinality/float(union_cardinality)\n",
    "    except ZeroDivisionError:\n",
    "        jac_score = 0.\n",
    " \n",
    "    return jac_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extend the jaccard_similarity function above to any number of inputs\n",
    "## which are specified as a list of lists.\n",
    "\n",
    "# Jaccard similarity is (among other things) a metric for measuring how well a model matches an observation to an \n",
    "# existing observation. This extends jaccard_similarity above to any number of inputs.\n",
    "## The essence of Jaccard Similarity is to find, for any two sets, the number of elements in the intersection divided by the number of elements \n",
    "## in the union of the sets.\n",
    "## From https://stackoverflow.com/questions/2541752/best-way-to-find-the-intersection-of-multiple-sets\n",
    "def jaccard_similarity_general(list_of_lists):\n",
    "    # Convert the lists into sets\n",
    "    set_list = [set(item) for item in list_of_lists]\n",
    "    intersection_cardinality = len(set.intersection(*set_list))\n",
    "    union_cardinality = len(set.union(*set_list))\n",
    "    \n",
    "    try:\n",
    "        jac_score = intersection_cardinality/float(union_cardinality)\n",
    "    except ZeroDivisionError:\n",
    "        jac_score = 0.\n",
    " \n",
    "    return jac_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the jaccard similarity between the search results returned for any query\n",
    "## This is a measure of the variation in the search results that are output for any single query string\n",
    "def intra_search_overlap(df_search_output, plot_title=''): \n",
    "    '''\n",
    "    Measure the jaccard similarity between the search results returned for any query.\n",
    "    This is a measure of the variation in the search results that are output for any single query string.\n",
    "    Display the intra-search result similarity as a heatmap.\n",
    "    \n",
    "    df_search_output is the result of a search which returns ONLY the 'CLIENT_QUESTION_PROCESSED' column.\n",
    "    '''\n",
    "    \n",
    "    j_scores_intra = []\n",
    "    for i in range(len(df_search_output)):\n",
    "        j_score_row = []\n",
    "        for j in range(len(df_search_output)):\n",
    "            j_score = jaccard_similarity(df_search_output.iloc[i].values[0], \n",
    "                                         df_search_output.iloc[j].values[0]\n",
    "                                        )\n",
    "            j_score_row.append(j_score)\n",
    "    \n",
    "        j_scores_intra.append(j_score_row)\n",
    "\n",
    "    # Put the jaccard scores in a dataframe for display using Seaborn\n",
    "    #df_display = pd.DataFrame(j_scores_intra, columns=list(range(0,len(df_search_output))))\n",
    "    grid_titles = [\"Search Result \" + str(i) for i in range(len(df_search_output))]\n",
    "    df_display = pd.DataFrame(j_scores_intra, columns=grid_titles)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.set_title(plot_title)\n",
    "    \n",
    "    sns.heatmap(df_display, \n",
    "                cmap=\"YlGnBu\", \n",
    "                annot=True, \n",
    "                annot_kws={\"size\": 9}, \n",
    "                linewidths=2, \n",
    "                linecolor='yellow', \n",
    "                yticklabels=grid_titles\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_search_overlap(df_search_ouputs, \n",
    "                         column_name,\n",
    "                         grid_titles, \n",
    "                         plot_title='Inter-Search Overlap of Core Topics'\n",
    "                        ):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Join the tokens returned by each search result into a single big list of tokens for that search query\n",
    "    search_output_tokens = []\n",
    "    for df_out in df_search_outputs:\n",
    "        ab = itertools.chain(df_out[column_name].values)\n",
    "        flat_ab = [item for sublist in list(ab) for item in sublist]\n",
    "        search_output_tokens.append(flat_ab)\n",
    "        \n",
    "    # for each pair of lists in search_output_tokens, get the jaccard distance\n",
    "    j_scores_inter = []\n",
    "    for i in range(len(search_output_tokens)):\n",
    "        j_score_row = []\n",
    "        for j in range(len(search_output_tokens)):\n",
    "            j_score = jaccard_similarity(search_output_tokens[i], search_output_tokens[j])\n",
    "            j_score_row.append(j_score)\n",
    "    \n",
    "        j_scores_inter.append(j_score_row)\n",
    "\n",
    "    # Put the jaccard scores in a dataframe for display using Seaborn\n",
    "    #df_display = pd.DataFrame(j_scores_inter, columns=list(range(0,len(search_output_tokens))))\n",
    "    df_display = pd.DataFrame(j_scores_inter, columns=grid_titles)\n",
    "    \n",
    "    #return df_display\n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.set_title(plot_title)\n",
    "    sns.heatmap(df_display, \n",
    "                cmap=\"BuPu\", \n",
    "                annot=True, \n",
    "                annot_kws={\"size\": 9}, \n",
    "                linewidths=2, \n",
    "                linecolor='yellow', \n",
    "                yticklabels=grid_titles) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word2Vec Models and Functions to Explore Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a gensim word2vec vectors object. \n",
    "## To access it and see the vectors, use the function display_vectors below\n",
    "def create_word2vec_model(n_gram_corpus, vector_size=100, context_window=5, alg=0, num_iter=30):\n",
    "    # Set workers = num of CPU cores on the machine\n",
    "    # alg = 1 uses SkipGram algorithm; alg = 0 uses CBOW algorithm\n",
    "    # iter sets the number of training epochs\n",
    "    t0 = time.time()\n",
    "    print(\"Creating the Word2Vec model with vector size {}\".format(vector_size))\n",
    "    print(\"This takes time...patience...\")\n",
    "    w2v_model = Word2Vec(n_gram_corpus, \n",
    "                         size=vector_size, \n",
    "                         window=context_window, \n",
    "                         min_count=20, \n",
    "                         sg=alg, \n",
    "                         workers=4, \n",
    "                         iter=num_iter\n",
    "                        )\n",
    "    t1 = time.time()\n",
    "    print(\"Word2Vec model with {} training eopchs was created in {} seconds.\".format(num_iter, round(t1-t0, 3)))\n",
    "    \n",
    "    # Use the model's KeyedVectors to reduce memory and delete the model\n",
    "    w2v_model_keyed_vecs = w2v_model.wv\n",
    "    del w2v_model\n",
    "    \n",
    "    # Number of words in the vocabulary\n",
    "    print(\"{} terms in the Word2Vec model's vocabulary.\".format(len(w2v_model_vecs.vocab)))\n",
    "    \n",
    "    return w2v_model_keyed_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Word2Vec model's keyed vectors\n",
    "def save_word2vec_model(model_keyed_vectors, file_name):\n",
    "    # model_keyed_vectors are saved from the output of create_word2vec_model above\n",
    "    t0 = time.time()\n",
    "    destination = os.path.join(intermediate_dir_path, file_name)\n",
    "    model_keyed_vectors.save(destination)\n",
    "    t1 = time.time()\n",
    "    print(\"Model save to {} in {} seconds\".format(destination, round(t1-t0, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Word2Vec model keyed vectors\n",
    "def load_word2vec_model(file_name):\n",
    "    # model_keyed_vectors are returned from create_word2vec_model above\n",
    "    t0 = time.time()\n",
    "    destination = os.path.join(intermediate_dir_path, file_name)\n",
    "    model_keyed_vectors = KeyedVectors.load(destination, mmap='r')\n",
    "    t1 = time.time()\n",
    "    print(\"Model loaded from {} in {} seconds\".format(destination, round(t1-t0, 3)))\n",
    "    return model_keyed_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the entire table of vector embedding values\n",
    "def display_vectors(w2v_KeyedVecs):\n",
    "    \n",
    "    #### NOTE: the full model is not used - only the model's KeyedVectors ####\n",
    "    # build a list of the terms, integer indices,\n",
    "    # and term counts from the given Word2Vec model vocabulary\n",
    "    ordered_vocab = [(term, voc.index, voc.count) for term, voc in w2v_KeyedVecs.vocab.items()]\n",
    "\n",
    "    # sort by the term counts, so the most common terms appear first\n",
    "    ordered_vocab = sorted(ordered_vocab, key=lambda item: -item[2])\n",
    "\n",
    "    # unzip the terms, integer indices, and counts into separate lists\n",
    "    ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "    # create a DataFrame with the food2vec vectors as data,\n",
    "    # and the terms as row labels\n",
    "    word_vectors = w2v_KeyedVecs.vectors[term_indices, :]\n",
    "    \n",
    "    # create a dataframe for displaying the vectors\n",
    "    df_display = pd.DataFrame(word_vectors, index=ordered_terms)\n",
    "    \n",
    "    return df_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Patrick Harrison and Radim Rahurek\n",
    "\n",
    "def pos_related_terms(w2v_KeyedVec, token, topn=20):\n",
    "    \n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for word, similarity in w2v_KeyedVec.most_similar(positive=[token], topn=topn):\n",
    "            print(u'{:20} {}'.format(word, round(similarity, 3)))\n",
    "    except KeyError:\n",
    "        print(\"Sorry, try a different term\")\n",
    "        \n",
    "def neg_related_terms(w2v_KeyedVec, token, topn=20):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for word, similarity in w2v_KeyedVec.most_similar(negative=[token], topn=topn):\n",
    "            print(u'{:20} {}'.format(word, round(similarity, 3)))\n",
    "    except KeyError:\n",
    "        print(\"Sorry, try a different term\")\n",
    "        \n",
    "\n",
    "def word_algebra(w2v_KeyedVec, add_string, subtract_string, topn=20):\n",
    "    \"\"\"\n",
    "    combine the vectors associated with the words provided\n",
    "    in add_string and subtract_string, look up the topn most similar\n",
    "    terms to the combined vector, and print the result(s)\n",
    "    Use add_string=None or '' or subtract=None or '' to leave the fields empty\n",
    "    \"\"\"\n",
    "    # Prep the strings\n",
    "    if add_string != None:\n",
    "        add = clean_doc(add_string)\n",
    "    else:\n",
    "        add = add_string\n",
    "        \n",
    "    if subtract_string != None:\n",
    "        subtract = clean_doc(subtract_string)\n",
    "    else:\n",
    "        subtract = subtract_string\n",
    "    \n",
    "    try:\n",
    "        answers = w2v_KeyedVec.most_similar(positive=add, negative=subtract, topn=topn)\n",
    "        for term, similarity in answers:\n",
    "            print(term)\n",
    "    except KeyError:\n",
    "        print(\"Sorry, one or more terms is not in the vocabulary - please try different terms.\")\n",
    "    \n",
    "        \n",
    "def odd_one_out(w2v_KeyedVec, token_string):\n",
    "    \n",
    "    token_list = clean_doc(token_string)\n",
    "    \n",
    "    try:\n",
    "        odd_one = w2v_KeyedVec.doesnt_match(token_list)\n",
    "    except ValueError:\n",
    "        odd_one = \"Sorry, one or more terms is not in the vocabulary - please try different terms.\"\n",
    "    \n",
    "    return odd_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Doc2Vec Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained doc2vec model given the file path\n",
    "import gensim.models as gm\n",
    "#### NOTE: The only pre-trained doc2vec model we have is 'doc2vec_pretrained'\n",
    "#### which points to 'Repos/Gartner/Doc2Vec-Pretrained-Vectors/doc2vec.bin'\n",
    "def load_doc2vec_pre_model(pre_trained_model_file):\n",
    "    # load the model\n",
    "    t0 = time.time()\n",
    "    doc2vec_model_pre = gm.Doc2Vec.load(pre_trained_model_file)\n",
    "    t1 = time.time()\n",
    "    print(\"Pre-trained Doc2Vec model {} loaded in {} seconds\".format(pre_trained_model_file, round(t1-t0, 2)))\n",
    "    \n",
    "    return doc2vec_model_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document vectors given a text corpus and a pre-trained doc2vec model\n",
    "def create_doc_vectors_pre(text_corpus, \n",
    "                           doc2vec_model_pre, \n",
    "                           start_alpha=0.001, \n",
    "                           infer_epoch=1000):\n",
    "    \n",
    "    # Model Hyper Parameters \n",
    "    # The lower the start_alpha and the higher the infer_epoch the longer\n",
    "    ## it takes to get the vectors and the more refined the vectors become\n",
    "    \n",
    "    doc2vec_vectors_pre = []\n",
    "    t0 = time.time()\n",
    "    print(\"Starting to vectorize corpus ... this can take a while ...\")\n",
    "    print(\"Using hyperparameters start_alpha = {} and infer_epoch = {}\".format(start_alpha, infer_epoch))\n",
    "    for d in text_corpus:\n",
    "        doc_vector = [str(x) for x in doc2vec_model_pre.infer_vector(d, \n",
    "                                                                     alpha=start_alpha, \n",
    "                                                                     steps=infer_epoch)]\n",
    "        doc_vector = [float(val) for val in doc_vector]\n",
    "        doc2vec_vectors_pre.append(doc_vector)\n",
    "    t1 = time.time()\n",
    "    print(\"Corpus vectors created using the pre-trained doc2vec model in {} seconds\".format(round(t1-t0, 2)))\n",
    "    print(\"Each document vector has length {}\".format(len(doc_vector)))\n",
    "    \n",
    "    return doc2vec_vectors_pre\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a doc2vec model on a given text corpus\n",
    "def create_doc2vec_model(text_corpus, \n",
    "                         vector_size, \n",
    "                         window_size=2, \n",
    "                         min_count=20, \n",
    "                         workers=4, \n",
    "                         epochs=500, \n",
    "                         tagged=0\n",
    "                        ):\n",
    "    # When tagged=1, just return the tagged text corpus\n",
    "    \n",
    "    # create the tagged corpus\n",
    "    print(\"Creating a tagged text corpus...\")\n",
    "    t0 = time.time()\n",
    "    text_corpus_tagged = [TaggedDocument(doc, [i]) for i, doc in enumerate(text_corpus)]\n",
    "    t1 = time.time()\n",
    "    print(\"Tagged text corpus created in {} seconds\".format(round(t1-t0, 2)))\n",
    "    \n",
    "    if tagged==1:\n",
    "        return text_corpus_tagged\n",
    "    \n",
    "    # Train the model\n",
    "    # Set workers = num of CPU cores on the machine\n",
    "    # skipgram or DBOW doesn't figure in this model\n",
    "    # epochs sets the number of training epochs\n",
    "    print(\"vector size = {}\".format(vector_size))\n",
    "    print(\"window size = {}\".format(window_size))\n",
    "    print(\"min count = {}\".format(min_count))\n",
    "    print(\"num of workers = {}\".format(workers))\n",
    "    print(\"num of training epochs = {}\".format(epochs))\n",
    "    print(\"Starting to create a trained Doc2Vec model...this takes time...\")\n",
    "    t2 = time.time()\n",
    "    d2v_model = Doc2Vec(text_corpus_tagged, \n",
    "                        vector_size=vector_size, \n",
    "                        window=window_size, \n",
    "                        min_count=min_count, \n",
    "                        workers=workers, \n",
    "                        epochs=epochs\n",
    "                       )\n",
    "    t3 = time.time()\n",
    "    print(\"Doc2Vec model created {} seconds\".format(round(t3-t2, 2)))\n",
    "    \n",
    "    return d2v_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a doc2vec model later use\n",
    "def save_doc2vec_model(model, destination):\n",
    "    model.save(destination)\n",
    "    # delete the temp training data\n",
    "    model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    print(\"Doc2Vec model {} saved to {}\".format(model, destination))\n",
    "    print(\"Load the model using load_doc2vec_model(destination)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a doc2vec model from its saved destination\n",
    "def load_doc2vec_model(destination):\n",
    "    t0 = time.time()\n",
    "    doc2vec_model = Doc2Vec.load(destination)\n",
    "    t1 = time.time()\n",
    "    print(\"Doc2Vec model from {} loaded in {} seconds\".format(destination, round(t1-t0, 2)))\n",
    "    \n",
    "    return doc2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors of a doc2vec model\n",
    "def doc2vec_get_vecs(doc2vec_n_grams_model):\n",
    "    return doc2vec_n_grams_model.docvecs.vectors_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LDA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lda_model(bow_corpus,\n",
    "                     num_topics,\n",
    "                     dictionary,\n",
    "                     workers=3,\n",
    "                     chunksize=200, \n",
    "                     passes=10, \n",
    "                     eval_every=2, \n",
    "                     iterations=500,  \n",
    "                     alpha='asymmetric', \n",
    "                     eta='auto'\n",
    "                    ):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    print(\"Creating LDA model with {} topics...this takes time...patience\".format(num_topics))\n",
    "    lda_model = LdaMulticore(bow_corpus,\n",
    "                             num_topics=num_topics,\n",
    "                             id2word=dictionary,\n",
    "                             workers=workers,\n",
    "                             chunksize=chunksize, \n",
    "                             passes=passes, \n",
    "                             eval_every=eval_every, \n",
    "                             iterations=iterations,  \n",
    "                             alpha=alpha,\n",
    "                             eta=eta, \n",
    "                             per_word_topics=True\n",
    "                            )\n",
    "    \n",
    "    t1=time.time()\n",
    "    print(\"LDA model with {} topics created in {} seconds.\".format(num_topics, round(t1-t0)))\n",
    "    \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lda_model(model, destination):\n",
    "    # Save the model for later use\n",
    "    model.save(destination)\n",
    "    print(\"The model {} is saved to {}\".format(model, destination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lda_model(destination):\n",
    "    print(\"LDA model loaded from {}\".format(destination))\n",
    "    return models.LdaModel.load(destination)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the coherence of the LDA model\n",
    "def lda_model_coherence(lda_model, doc_corpus, dictionary, coherence=\"c_v\"):\n",
    "    t0 = time.time()\n",
    "    print(\"Starting to calculate coherence score...this can take some time...\")\n",
    "    coherence = CoherenceModel(model=lda_model, texts=doc_corpus, dictionary=dictionary, coherence=coherence)\n",
    "    coherence_score = coherence.get_coherence()\n",
    "    t1 = time.time()\n",
    "    print(\"Coherence score = {} calculated in {} seconds.\".format(coherence_score, round(t1-t0,3)))\n",
    "    return coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity of a topic model\n",
    "## The lower the perplexity, the better the model\n",
    "## From https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "\n",
    "def lda_model_perplexity(lda_model, bow_corpus):\n",
    "    t0 = time.time()\n",
    "    print(\"Starting to calculate coherence score...this can take some time...\")\n",
    "    perplexity_score = lda_model.log_perplexity(bow_corpus)\n",
    "    t1 = time.time()\n",
    "    print(\"Perplexity score = {} calculated in {} seconds.\".format(perplexity_score, round(t1-t0,3)))\n",
    "    return perplexity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best combination of hyperparameters for an LDA n-gram model\n",
    "\n",
    "Inspired by https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lda_model(n_gram_bow_corpus, n_gram_dictionary, n_gram_doc_corpus, num_combinations=2):\n",
    "    # Hyperparameters of the LDA model\n",
    "    num_topics = [5, 10, 15, 20]\n",
    "    alpha = [0.01, 0.31, 0.61, 0.90, 'symmetric', 'asymmetric']\n",
    "    eta = [0.01, 0.31, 0.61, 0.90, 'symmetric', 'auto']\n",
    "    \n",
    "    # Create m hyperparameter lists\n",
    "    m = 10_000\n",
    "    hyp_param_lists = [[random.choice(num_topics), random.choice(alpha), random.choice(eta)] for i in range(m)]\n",
    "    \n",
    "    # Try out num_combinations different combinations of hyperparameter settings\n",
    "    hyp_params = random.sample(remove_duplicate_lists(hyp_param_lists), num_combinations)\n",
    "    print(\"The hyperparameter lists are \\n {}\".format(hyp_params))\n",
    "    \n",
    "    # Create a bunch of n_gram LDA models and evaluate their coherence\n",
    "    tuning_vals = []\n",
    "    for hyp in hyp_params:\n",
    "        hyp_vals = hyp\n",
    "        print(\"Creating an LDA model with hyperparameters {}\".format(hyp_vals))\n",
    "        lda_model = create_lda_model(n_gram_bow_corpus, hyp[0], n_gram_dictionary, alpha=hyp[1], eta=hyp[2])\n",
    "        coherence_val = lda_model_coherence(lda_model, n_gram_doc_corpus, n_gram_dictionary)\n",
    "        hyp_vals.append(coherence_val)\n",
    "        perplex_val = lda_model_perplexity(lda_model, n_gram_bow_corpus)\n",
    "        hyp_vals.append(perplex_val)\n",
    "        print(\"Hyperparameter settings, coherence, and perplexity values = {}\".format(hyp_vals))\n",
    "        tuning_vals.append(hyp_vals)\n",
    "    \n",
    "    df_tuning_vals = pd.DataFrame(tuning_vals, columns=['Num Topics', 'alpha', 'eta', 'Coherence', 'Perplexity'])\n",
    "    return df_tuning_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the NMF topic model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# scikit learn model persistence -- saving and loading\n",
    "#from sklearn.externals import joblib\n",
    "# DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and \n",
    "## will be removed in 0.23. Please import this functionality directly from joblib, \n",
    "## which can be installed with: pip install joblib. If this warning is raised when \n",
    "## loading pickled models, you may need to \n",
    "## re-serialize those models with scikit-learn 0.21+.\n",
    "\n",
    "# joblib.dump(scikit_model, 'filename.pkl') # save the model\n",
    "# scikit_model = joblib.load('filename.pkl') # load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(topic_model, topic_number, topn=20):\n",
    "    \"\"\"\n",
    "    accept a user-supplied nlp_model and topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "        \n",
    "    print(u'{:20} {}'.format(u'Term', u'Frequency'))\n",
    "\n",
    "    for term, frequency in topic_model.show_topic(topic_number, topn=topn):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "## https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df\n",
    "\n",
    "def get_lda_topics(lda_model, num_topics, top_n=20):\n",
    "    '''\n",
    "    Show the words that make up the topics in an LDA topic model\n",
    "    '''\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = lda_model.show_topic(i, topn = top_n);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i)] = [i[0] for i in words];\n",
    "    \n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given LDA model, get the dominant topic for each document in the corpus\n",
    "def get_dominant_topics(lda_n_grams_model, bow_n_grams):\n",
    "    t0 = time.time()\n",
    "    corpus_info = lda_n_grams_model[bow_n_grams]\n",
    "    t1 = time.time()\n",
    "    print(\"Got topic model information for the entire corpus in {} seconds.\".format(round(t1-t0, 3)))\n",
    "    print(\"Starting to get the dominant topic for each document in the corpus...this takes a while...patience...\")\n",
    "    t2 = time.time()\n",
    "    dom_topics_and_probs = [[sorted(corpus_info[n][0], key=lambda x: x[1], reverse=True)[0], n] for n in range(len(corpus_info))]\n",
    "    t3 = time.time()\n",
    "    print(\"Got dominant topic for each document in the corpus in {} seconds.\".format(round(t3-t2, 3)))\n",
    "    \n",
    "    return dom_topics_and_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a document number and an LDA model, get the document's dominant topic number\n",
    "## the topic number's dominance probability, and the raw of the document.\n",
    "def get_dominant_topic(doc_num, lda_n_grams_model, bow_n_grams, raw_text_corpus):\n",
    "    doc_info = lda_n_grams_model[bow_n_grams[doc_num]]\n",
    "    #print(\"doc_info {}\".format(doc_info))\n",
    "    dom_topic_and_prob = sorted(doc_info[0], key=lambda x: x[1], reverse=True)[0]\n",
    "    dom_topic = dom_topic_and_prob[0]\n",
    "    #print(\"dom_topic {}\".format(dom_topic))\n",
    "    dom_topic_prob = dom_topic_and_prob[1]\n",
    "    dom_topic_keywords = lda_n_grams_model.show_topic(dom_topic, 10)\n",
    "    dom_topic_keywords_list = [x[0] for x in dom_topic_keywords]\n",
    "    doc_text = raw_text_corpus[doc_num]\n",
    "    \n",
    "    df_dom_results= pd.DataFrame([[dom_topic, dom_topic_prob, dom_topic_keywords_list, doc_text]], \n",
    "                                 columns=['Dominant Topic', 'Probability', 'Topic Keywords', 'Document Text'])\n",
    "    \n",
    "    return df_dom_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a topic number and the dominant topics for an LDA model from get_dominant_topics above, \n",
    "## get the topic's r most representative documents\n",
    "def get_representative_docs(topic_num, dominant_topics_corpus, raw_text_corpus, num_display=5):\n",
    "    # dominant_topics_corpus is the output of the get_dominant_topics function above\n",
    "    \n",
    "    # find the ones that match the topic number\n",
    "    rel_docs = [el for el in dominant_topics_corpus if el[0][0] == topic_num]\n",
    "    #print(rel_docs)\n",
    "    # then sort them from higest to lowest to find the most representative docs for the given topic\n",
    "    repr_docs = sorted(rel_docs, key=lambda x: x[0][1], reverse=True)[0:num_display]\n",
    "    #print(repr_docs)\n",
    "    \n",
    "    # Pull the results together into a dataframe\n",
    "    results = [[rep[1], rep[0][1], raw_text_corpus[rep[1]]] for rep in repr_docs]\n",
    "    df_results = pd.DataFrame(results, columns=['Document Number', 'Probability of Topic for Document', 'Document Text'])\n",
    "    print(\"Here are top {} documents where topic {} is dominant.\".format(num_display, topic_num))\n",
    "    \n",
    "    return df_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df\n",
    "\n",
    "def get_nmf_topics(nmf_model, num_topics, n_top_words=20):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    vectorizer = CountVectorizer(analyzer='word')\n",
    "    vectorizer._validate_vocabulary()\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = nmf_model.components_[i].argsort()[:-n_top_words - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words\n",
    "    \n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "# Finding the dominant topic in any document\n",
    "def format_topics_sentences(topic_model, corpus, texts):\n",
    "    # Initialize the output dataframe\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(topic_model[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = topic_model.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', '% Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "#### And here's how to get the dominant topics and display them ####\n",
    "#df_topic_sents_keywords = format_topics_sentences(topic_model, corpus, texts)\n",
    "\n",
    "# Format\n",
    "#df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "#df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "#df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "# Find the most representative document for each topic\n",
    "# Group top 5 sentences under each topic\n",
    "#sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "#sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "#for i, grp in sent_topics_outdf_grpd:\n",
    "#    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "#                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "#                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "#sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "#sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "#sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "# Number of Documents for Each Topic\n",
    "#topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "#topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "#topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "#df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "#df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "#df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep to display an LDA model using pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis for visualizing topic models\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim ###DEPRECATED##\n",
    "# Use this instead - see https://stackoverflow.com/questions/66759852/no-module-named-pyldavis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsubrama\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Create LDA viz content\n",
    "def prep_lda_viz(n_gram_lda_model, n_gram_bow, n_gram_dictionary):\n",
    "    print(\"This can take a while...patience...\")\n",
    "    t0 = time.time()\n",
    "    lda_viz_content = gensimvis.prepare(n_gram_lda_model, n_gram_bow, n_gram_dictionary)\n",
    "    t1 = time.time()\n",
    "    print(\"LDA Viz prep data created in {} seconds.\".format(round(t1-t0, 3)))\n",
    "    return lda_viz_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize (save and load) LDAvis content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsubrama\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Serialize the pyLDAvis prepared file to disk\n",
    "def save_lda_viz_content(lda_viz_content, file_path):\n",
    "     with open(file_path, 'wb') as f:\n",
    "        pickle.dump(lda_viz_content, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsubrama\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# load the pre-prepared ldavis_content from disk\n",
    "def load_lda_viz_content(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        lda_viz_content = pickle.load(f)\n",
    "        \n",
    "    return lda_viz_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show LDA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsubrama\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def show_lda_viz(lda_viz_content): \n",
    "    return pyLDAvis.display(lda_viz_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsubrama\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Import the Scikit Learn t-SNE model\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsubrama\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def prep_tsne_input(keyed_vectors, num_words=1000):\n",
    "    \n",
    "    '''\n",
    "    Take a set of KeyedVectors produced by a Word2Vec or Doc2Vec model and prep it \n",
    "    for input into Scikit Learn's TSNE model.\n",
    "    \n",
    "    num_words cuts down the complexity by selecting a subset of words from the vocabulary \n",
    "    (the num_words most frequent)\n",
    "    '''\n",
    "    t0 = time.time()\n",
    "    df_vecs = display_vectors(keyed_vectors)\n",
    "    t1 = time.time()\n",
    "    print(\"t-SNE input dataframe created in {:.2f} secs.\".format(t1-t0))\n",
    "    print(\"Creating t-SNE vectors ... this will take some time ...\")\n",
    "    \n",
    "    # df_vecs is the input to the t-SNE model\n",
    "    tsne = TSNE()\n",
    "    tsne_input = df_vecs.head(num_words)\n",
    "    t2 = time.time()\n",
    "    tsne_vectors = tsne.fit_transform(tsne_input.values)\n",
    "    t3 = time.time()\n",
    "    print(\"t-SNE vectors created in {:.2f} secs.\".format(t3-t2))\n",
    "    \n",
    "    # Convert the tsne_vectors into a dataframe\n",
    "    # These can then be used to visualize t-SNE using Bokeh\n",
    "    df_tsne_vectors = pd.DataFrame(tsne_vectors,\n",
    "                                   index=pd.Index(tsne_input.index),\n",
    "                                   columns=[u'x_coord', u'y_coord']\n",
    "                                  )\n",
    "    \n",
    "    return df_tsne_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsubrama\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "def plot_tsne(df_tsne_vectors, dot_color='blue', title_add=''):\n",
    "    '''\n",
    "    Visualize the t-SNE vectors using Bokeh.\n",
    "    '''\n",
    "    \n",
    "    # Map the vocabulary to the t-SNE vectors\n",
    "    df_tsne_vectors[u'word'] = df_tsne_vectors.index\n",
    "    \n",
    "    # add df_tsne_vectors as a ColumnDataSource for Bokeh\n",
    "    plot_data = ColumnDataSource(df_tsne_vectors)\n",
    "    \n",
    "    # create the plot and configure the\n",
    "    ## title, dimensions, and tools\n",
    "    tsne_plot = figure(title=u't-SNE Word Embeddings of Corpus' + title_add,\n",
    "                       plot_width = 800,\n",
    "                       plot_height = 800,\n",
    "                       tools= (u'pan, wheel_zoom, box_zoom,'\n",
    "                               u'box_select, reset'),\n",
    "                       active_scroll=u'wheel_zoom')\n",
    "\n",
    "    # add a hover tool to display words on roll-over\n",
    "    tsne_plot.add_tools( HoverTool(tooltips = u'@word') )\n",
    "\n",
    "    # draw the words as circles on the plot\n",
    "    tsne_plot.circle(u'x_coord', u'y_coord', source=plot_data,\n",
    "                     color=dot_color, line_alpha=0.2, fill_alpha=0.1,\n",
    "                     size=10, hover_line_color=u'black')\n",
    "\n",
    "    # configure visual elements of the plot\n",
    "    tsne_plot.title.text_font_size = value(u'16pt')\n",
    "    tsne_plot.xaxis.visible = False\n",
    "    tsne_plot.yaxis.visible = False\n",
    "    tsne_plot.grid.grid_line_color = None\n",
    "    tsne_plot.outline_line_color = None\n",
    "\n",
    "    # Display the plot\n",
    "    show(tsne_plot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsubrama\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Utility function that combines prep_tsne_input and tsne_plot above\n",
    "def show_tsne(nlp_n_gram_model, num_words=1000, title_add=\"\", dot_color=\"magenta\"):\n",
    "    # n_gram_model is for example doc2vec_model_2_grams\n",
    "    # num_words sets a limit on the number of words considered -- lower the number, faster the plotting\n",
    "    \n",
    "    # Prep the data for the t-SNE plot\n",
    "    ## NOTE: nlp_n_gram_model.wv produces the keyed vectors object for the nlp_model \n",
    "    ## both for doc2vec and word2vec\n",
    "    df_tsne = prep_tsne_input(nlp_n_gram_model.wv, num_words=num_words)\n",
    "    \n",
    "    # Plot the t-SNE using Bokeh\n",
    "    plot_tsne(df_tsne, dot_color=dot_color, title_add=title_add)\n",
    "    \n",
    "    return df_tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing NLP Model Vector Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsubrama\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#### TO DO: Modify to handle Word2Vec models ####\n",
    "# Assume you have the vectors from a doc2vec or word2vec model. We want to take the vectors and see how they cluster.\n",
    "## Here we set up the number of clusters, run a K-Means model and see how the vectors fall into the various clusters or groups.\n",
    "## We then take each vector, reduce it to 2 dimensions using Principal Component Analysis (PCA) and plot the clusters\n",
    "## Requires Imports ##\n",
    "# from sklearn.cluster import KMeans\n",
    "## Modified from https://medium.com/@ermolushka/text-clusterization-using-python-and-doc2vec-8c499668fa61\n",
    "#### NOTE: Plotting will return an error if a centroid has no data points attached to it. \n",
    "#### This can happen when the number of clusters is too high. The error can be avoided by plotting a simple matplotlib plot: together=2\n",
    "\n",
    "def show_vec_clusters(nlp_n_grams_vecs, num_clusters, together=1):\n",
    "    # nlp_n_grams_vecs are the vectors of an nlp model \n",
    "    ## which can be doc2vec or word2vec\n",
    "    ## nlp_n_grams_vecs = doc2vec_n_grams_model.docvecs.vectors_docs\n",
    "    ## word2vec models directly return the vecs \n",
    "    # together = 1 shows all clusters together \n",
    "    # together = 0 plots each cluster separately\n",
    "    # any other value of together produces a simple matplotlib plot where the colors can be customized\n",
    "    \n",
    "    t0 = time.time()\n",
    "    print(\"Creating a K-Means model for the document vectors...\")\n",
    "    kmeans_model = KMeans(n_clusters=num_clusters, init=\"k-means++\", max_iter=1000) \n",
    "    X = kmeans_model.fit(nlp_n_grams_vecs)\n",
    "    labels=kmeans_model.labels_.tolist()\n",
    "    centroids = kmeans_model.cluster_centers_\n",
    "    l = kmeans_model.fit_predict(nlp_n_grams_vecs)\n",
    "    t1 = time.time()\n",
    "    print(\"K-Means model created in {} seconds\".format(round(t1-t0)))\n",
    "    \n",
    "    # Reduce the dimensionality of the vectors\n",
    "    t2 = time.time()\n",
    "    print(\"Reducing the dimensionality of the vectors in the corpus...\")\n",
    "    pca = PCA(n_components=2).fit(nlp_n_grams_vecs)\n",
    "    datapoints = pca.transform(nlp_n_grams_vecs)\n",
    "    centroidpoints = pca.transform(centroids)\n",
    "    t3 = time.time()\n",
    "    print(\"Reduced dimensionality of the corpus vectors in {} seconds\".format(round(t3-t2)))\n",
    "    #print(\"centroidpoints = {}\".format(centroidpoints))\n",
    "    \n",
    "    # Create dataframe for plotting in Seaborn\n",
    "    t4 = time.time()\n",
    "    print(\"Putting the data in the right format for plotting...\")\n",
    "    datapoints_labeled = [np.append(labels[i], datapoints[i]) for i in range(len(datapoints))]\n",
    "    df_c = pd.DataFrame(datapoints_labeled, columns=[\"Group\", \"x\", \"y\"])\n",
    "    # Change the cluster numbers from floats to integers\n",
    "    df_c[\"Group\"] = df_c[\"Group\"].astype(int)\n",
    "    t5 = time.time()\n",
    "    print(\"Data formatted for plotting in {} seconds\".format(round(t5-t4)))\n",
    "    \n",
    "    t6 = time.time()\n",
    "    if together == 1:\n",
    "        #plot data with seaborn - all clusters together\n",
    "        print(\"Starting to plot...\")\n",
    "        facet = sns.lmplot(data=df_c, x='x', y='y', hue='Group', height=6, aspect=2, fit_reg=False, legend=True, legend_out=False)\n",
    "        # Uncomment these limits when plotting doc2vec or word2vec vectors that are within a tighter range\n",
    "        #facet.set(xlim=(-10, 10))\n",
    "        #facet.set(ylim=(-10,10))\n",
    "        # Plot the centroids and name them\n",
    "        for i in range(len(centroidpoints)):\n",
    "            plt.text(centroidpoints[i,0]+0.05, centroidpoints[i,1]+0.05, str(i), fontsize=12, fontweight=\"bold\")\n",
    "            plt.scatter(centroidpoints[i,0], centroidpoints[i,1], marker='^', color='black', s=100);\n",
    "    elif together == 0:\n",
    "        # plot each cluster separately\n",
    "        for i in range(len(centroidpoints)):\n",
    "            facet = sns.lmplot(data=df_c[df_c['Group']==i], x='x', y='y', hue='Group', height=4, aspect=2, fit_reg=False, legend=False, legend_out=False)\n",
    "            # Uncomment these limits when plotting doc2vec or word2vec vectors that are within a tighter range\n",
    "            #facet.set(xlim=(-10, 10))\n",
    "            #facet.set(ylim=(-10,10))\n",
    "            facet.fig.suptitle(\"Group \" + str(i))\n",
    "            for i in range(len(centroidpoints)):\n",
    "                plt.text(centroidpoints[i,0]+0.05, centroidpoints[i,1]+0.05, str(i), fontsize=12, fontweight=\"bold\")\n",
    "                plt.scatter(centroidpoints[i,0], centroidpoints[i,1], marker='^', color='black', s=100); \n",
    "    else:\n",
    "        # Simple matplotlib plot\n",
    "        fig, ax = plt.subplots(figsize=(10,8))\n",
    "        colors = [\"darkorange\", \"green\", \"magenta\", \"violet\", \"aqua\", \"lime\", \"yellow\", \"dodgerblue\", \"fuchsia\", \"maroon\", \"chocolate\", \"darksalmon\"]\n",
    "        label1 = colors[0:num_clusters]\n",
    "        color = [label1[i] for i in labels]\n",
    "        plt.scatter(datapoints[:, 0], datapoints[:, 1], c=color)\n",
    "        plt.scatter(centroidpoints[:, 0], centroidpoints[:, 1], marker=\"^\", s=150, c=\"#000000\")\n",
    "        plt.show();\n",
    "\n",
    "    t7 = time.time()\n",
    "    print(\"Plot complete in {} seconds\".format(round(t7-t6)))\n",
    "    print(\"Model to plot in {} seconds\".format(round(t7-t0)))\n",
    "    \n",
    "    # Return the classification labels of each of the vectors for further analysis\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsubrama\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#### TO DO: Modify to handle Word2Vec models ####\n",
    "\n",
    "# Assume you have a doc2vec or word2vec model. We want to take the vectors and see how they cluster.\n",
    "## Here we set up the number of clusters, run a K-Means model and see how the vectors fall into the various clusters or groups.\n",
    "## We then take each vector, reduce it to 2 dimensions using Principal Component Analysis (PCA) and plot the clusters\n",
    "## Requires Imports ##\n",
    "# from sklearn.cluster import KMeans\n",
    "## Modified from https://medium.com/@ermolushka/text-clusterization-using-python-and-doc2vec-8c499668fa61\n",
    "\n",
    "def show_clusters(doc2vec_n_gram_model, num_clusters, together=1):\n",
    "    # together = 1 shows all clusters together \n",
    "    # together = 0 plots each cluster separately\n",
    "    # any other value of together produces a simple matplotlib plot where the colors can be customized\n",
    "    \n",
    "    t0 = time.time()\n",
    "    print(\"Creating a K-Means model for the document vectors...\")\n",
    "    kmeans_model = KMeans(n_clusters=num_clusters, init=\"k-means++\", max_iter=1000) \n",
    "    X = kmeans_model.fit(doc2vec_n_gram_model.docvecs.vectors_docs)\n",
    "    labels=kmeans_model.labels_.tolist()\n",
    "    centroids = kmeans_model.cluster_centers_\n",
    "    l = kmeans_model.fit_predict(doc2vec_n_gram_model.docvecs.vectors_docs)\n",
    "    t1 = time.time()\n",
    "    print(\"K-Means model created in {} seconds\".format(round(t1-t0)))\n",
    "    \n",
    "    # Reduce the dimensionality of the vectors\n",
    "    t2 = time.time()\n",
    "    print(\"Reducing the dimensionality of the vectors in the corpus...\")\n",
    "    pca = PCA(n_components=2).fit(doc2vec_n_gram_model.docvecs.vectors_docs)\n",
    "    datapoints = pca.transform(doc2vec_n_gram_model.docvecs.vectors_docs)\n",
    "    centroidpoints = pca.transform(centroids)\n",
    "    t3 = time.time()\n",
    "    print(\"Reduced dimensionality of the corpus vectors in {} seconds\".format(round(t3-t2)))\n",
    "    \n",
    "    # Create dataframe for plotting in Seaborn\n",
    "    t4 = time.time()\n",
    "    print(\"Putting the data in the right format for plotting...\")\n",
    "    datapoints_labeled = [np.append(labels[i], datapoints[i]) for i in range(len(datapoints))]\n",
    "    df_c = pd.DataFrame(datapoints_labeled, columns=[\"Group\", \"x\", \"y\"])\n",
    "    # Change the cluster numbers from floats to integers\n",
    "    df_c[\"Group\"] = df_c[\"Group\"].astype(int)\n",
    "    t5 = time.time()\n",
    "    print(\"Data formatted for plotting in {} seconds\".format(round(t5-t4)))\n",
    "    \n",
    "    t6 = time.time()\n",
    "    if together == 1:\n",
    "        #plot data with seaborn - all clusters together\n",
    "        print(\"Starting to plot...\")\n",
    "        facet = sns.lmplot(data=df_c, x='x', y='y', hue='Group', \n",
    "                           fit_reg=False, legend=True, legend_out=False, \n",
    "                           height=6, aspect=2)\n",
    "        facet.set(xlim=(-10, 10))\n",
    "        facet.set(ylim=(-10,10))\n",
    "        # Plot the centroids and name them\n",
    "        for i in range(len(centroidpoints)):\n",
    "            plt.text(centroidpoints[i,0]+0.05, centroidpoints[i,1]+0.05, str(i), fontsize=12, fontweight=\"bold\")\n",
    "            plt.scatter(centroidpoints[i,0], centroidpoints[i,1], marker='^', color='black', s=100);\n",
    "    elif together == 0:\n",
    "        # plot each cluster separately\n",
    "        for i in range(len(centroidpoints)):\n",
    "            facet = sns.lmplot(data=df_c[df_c['Group']==i], x='x', y='y', hue='Group', height=4, aspect=2,  \n",
    "                               fit_reg=False, legend=False, legend_out=False)\n",
    "            facet.set(xlim=(-10, 10))\n",
    "            facet.set(ylim=(-10,10))\n",
    "            facet.fig.suptitle(\"Group \" + str(i))\n",
    "            for i in range(len(centroidpoints)):\n",
    "                plt.text(centroidpoints[i,0]+0.05, centroidpoints[i,1]+0.05, str(i), fontsize=12, fontweight=\"bold\")\n",
    "                plt.scatter(centroidpoints[i,0], centroidpoints[i,1], marker='^', color='black', s=100); \n",
    "    else:\n",
    "        # Simple matplotlib plot\n",
    "        fig, ax = plt.subplots(figsize=(10,8))\n",
    "        colors = [\"darkorange\", \"green\", \"magenta\", \"violet\", \"aqua\", \"lime\", \"yellow\", \"dodgerblue\", \"fuchsia\", \"maroon\", \"chocolate\", \"darksalmon\"]\n",
    "        label1 = colors[0:num_clusters]\n",
    "        color = [label1[i] for i in labels]\n",
    "        plt.scatter(datapoints[:, 0], datapoints[:, 1], c=color)\n",
    "        plt.scatter(centroidpoints[:, 0], centroidpoints[:, 1], marker=\"^\", s=150, c=\"#000000\")\n",
    "        plt.show();\n",
    "\n",
    "    t7 = time.time()\n",
    "    print(\"Plot complete in {} seconds\".format(round(t7-t6)))\n",
    "    print(\"Model to plot in {} seconds\".format(round(t7-t0)))\n",
    "    \n",
    "    # Return the classification labels of each of the vectors for further analysis\n",
    "    return labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
