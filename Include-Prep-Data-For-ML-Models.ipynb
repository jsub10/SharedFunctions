{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include-Prep-Data-For-ML-Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is a single longish function that creates inputs for any ML model ####\n",
    "#### For unsupervised models, the entire dataset is returned ####\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import math\n",
    "\n",
    "def create_model_input(df_dataset, \n",
    "                       target_feature,\n",
    "                       target_feature_type, \n",
    "                       categorical_features_list, \n",
    "                       numerical_features_list, \n",
    "                       scaler='Standard'\n",
    "                      ):\n",
    "    \n",
    "    # df_dataset is a complete pre-processed input dataset, e.g., df_ig\n",
    "    # target_feature is the feature that is the target for the model\n",
    "    #### NOTE: if target_feature is '', then the entire dataset is returned after been one-hot encoded and scaled ####\n",
    "    # target_type is 'categorical' or 'numerical' or '' (when there is no target specified)\n",
    "    # categorical_features_list is the list of categorical features used by the model \n",
    "    #  (will include the target feature if the target feature is categorical)\n",
    "    # numerical_features_list is the list of numerical features used by the model\n",
    "    #  (will include the target feature if the target feature is numerical)\n",
    "    # scaler can be 'Standard' (default) or 'MinMax'\n",
    "    \n",
    "    #### Step 1: Decide how to split the dataset into train, validate, and test datasets ####\n",
    "    # VAL_PCT_SPLIT can be set to 0.0 if needed\n",
    "    TRAIN_PCT_SPLIT = 0.8\n",
    "    VAL_PCT_SPLIT = 0.0\n",
    "    TEST_PCT_SPLIT = 0.2\n",
    "    \n",
    "    #### Step 2: Separate the target feature from the other features ####\n",
    "    if target_feature != '':\n",
    "        categorical_features = [x for x in categorical_features_list if x != target_feature]\n",
    "        numerical_features = [x for x in numerical_features_list if x != target_feature]\n",
    "    else:\n",
    "        categorical_features = categorical_features_list\n",
    "        numerical_features = numerical_features_list\n",
    "    \n",
    "    #### Step 3: Create a dataset with the requisite features for the model from the full dataset ####\n",
    "    if target_feature != '':\n",
    "        df_model = df_dataset[categorical_features + numerical_features + [target_feature]]\n",
    "    else:\n",
    "        df_model = df_dataset[categorical_features + numerical_features]\n",
    "    \n",
    "    #### Step 4: One-hot-encode the categorical features ####\n",
    "    df_model = pd.get_dummies(df_model, columns=categorical_features)\n",
    "    \n",
    "    #### Step 5: Label encode the target feature if it's a categorical feature ####\n",
    "    if target_feature_type == 'categorical':\n",
    "        le = LabelEncoder()\n",
    "        df_model[target_feature] = le.fit_transform(df_model[target_feature])\n",
    "        \n",
    "    #### df_model now contains all the features and the target we need\n",
    "    ####  in addition, df_model has its categorical features one-hot-encoded and \n",
    "    ####  its label/target encoded if needed\n",
    "    \n",
    "    #### Step 6: Shuffle the dataset and split it into train, val, and test ####\n",
    "    # Shuffle the one-hot-encoded and label-encoded dataset\n",
    "    df_shuff = shuffle(df_model, random_state=42) # set seed for replicability\n",
    "    # Why 42? It's the answer to the \"ultimate question of life, the universe, and everything\" as worked out by \n",
    "    ## the supercomputer Deep Thought in Douglas Adams' The Hitchiker's Guide to the Universe.\n",
    "    \n",
    "    (num_rows, num_cols) = df_shuff.shape\n",
    "    \n",
    "    num_train = math.floor(TRAIN_PCT_SPLIT * num_rows)\n",
    "    num_val = math.floor(VAL_PCT_SPLIT * num_rows)\n",
    "    # num_test consists of the remaning rows of the dataset\n",
    "    num_test = num_rows - (num_train + num_val)\n",
    "    \n",
    "    # Train, val, and test dataframes\n",
    "    df_train = df_shuff.iloc[0:num_train]\n",
    "    df_val = df_shuff.iloc[num_train:num_train+num_val]\n",
    "    df_test = df_shuff.iloc[num_train+num_val: ]\n",
    "    \n",
    "    # df_val_test combines df_val and df_test in case we don't need them separately\n",
    "    # . e.g., when using k-fold cross validation with a scikit classifier\n",
    "    # Typically used when the dataset is small\n",
    "    df_val_test = pd.concat([df_val, df_test], axis=0)\n",
    "    \n",
    "    # Use df_train_val to (re)train the optimal model once the optimal model \n",
    "    #  has been determined using grid search\n",
    "    df_train_val = pd.concat([df_train, df_val], axis=0)\n",
    "    \n",
    "    # And finally, this is the entire dataset (for unsupervised learning, e.g., clustering analysis)\n",
    "    df_full = pd.concat([df_train_val, df_test], axis=0)\n",
    "     \n",
    "    #### Step 8: Scale the numerical features OVER THE TRAINING DATASET ONLY ####\n",
    "    if scaler == 'Standard':\n",
    "        sc = StandardScaler()\n",
    "    elif scaler == 'MinMax':\n",
    "        sc = MinMaxScaler()\n",
    "    else:\n",
    "        sc = StandardScaler() # use StandardScaler as the default scaler\n",
    "    \n",
    "    #### NOTE: a copy is made to aviod the pandas SettingWithCopying warning ####\n",
    "    #### See https://www.dataquest.io/blog/settingwithcopywarning/ ####\n",
    "    if target_feature == '':\n",
    "        # Scale the entire dataset's numerical features\n",
    "        df_full_scaled = df_full.copy()\n",
    "        df_full_scaled[numerical_features] = sc.fit_transform(df_full[numerical_features])\n",
    "    else:\n",
    "        df_full_scaled = df_full\n",
    "    \n",
    "    # Scale just the training dataset and use these scaler values to scale the val and test datasets\n",
    "    df_train_scaled = df_train.copy()\n",
    "    df_train_scaled[numerical_features] = sc.fit_transform(df_train[numerical_features])\n",
    "\n",
    "    \n",
    "    #### Step 9: Scale the numerical features of the other datasets using the scaler values\n",
    "    ####  of the training dataset ####\n",
    "    #### NOTE: a copy is made to aviod the pandas SettingWithCopying warning ####\n",
    "    #### See https://www.dataquest.io/blog/settingwithcopywarning/ ####\n",
    "    \n",
    "    # Check to make sure that the validation slice % is not 0\n",
    "    if len(df_val) > 0:\n",
    "        df_val_scaled = df_val.copy()\n",
    "        df_val_scaled[numerical_features] = sc.transform(df_val[numerical_features])\n",
    "    else:\n",
    "        df_val_scaled = df_val\n",
    "    \n",
    "    df_test_scaled = df_test.copy()\n",
    "    df_test_scaled[numerical_features] = sc.transform(df_test[numerical_features])\n",
    "    \n",
    "    df_val_test_scaled = df_val_test.copy()\n",
    "    df_val_test_scaled[numerical_features] = sc.transform(df_val_test[numerical_features])\n",
    "    \n",
    "    df_train_val_scaled = df_train_val.copy()\n",
    "    df_train_val_scaled[numerical_features] = sc.transform(df_train_val[numerical_features])\n",
    "    \n",
    "    #### Step 10: Get the targets for SciKit Learn models as a (num, ) shape array of reals ####\n",
    "    #### there are no y values for the full dataset becuause there is no target ####\n",
    "    if target_feature != '':\n",
    "        y_train = df_train_scaled[target_feature].values.astype('float32')\n",
    "        y_val = df_val_scaled[target_feature].values.astype('float32')\n",
    "        y_test = df_test_scaled[target_feature].values.astype('float32')\n",
    "        y_val_test = df_val_test_scaled[target_feature].values.astype('float32')\n",
    "        y_train_val = df_train_val_scaled[target_feature].values.astype('float32')\n",
    "    else:\n",
    "        y_train = []\n",
    "        y_val = []\n",
    "        y_test = []\n",
    "        y_val_test = []\n",
    "        y_train_val = []\n",
    "    \n",
    "    #### Step 11: Create the input and target arrays ####\n",
    "    # Get the feature array as it currently exists for the df_prepped_dataset\n",
    "    #### NOTE: The feature names may have changed when the categorical features\n",
    "    #### are one-hot-encoded\n",
    "    # So features are now all column names EXCEPT for the Target\n",
    "    features_list = list(df_train_scaled)\n",
    "    if target_feature != '':\n",
    "        features_list.remove(target_feature)\n",
    "    \n",
    "    X_train = df_train_scaled[features_list].values\n",
    "    X_val = df_val_scaled[features_list].values\n",
    "    X_test = df_test_scaled[features_list].values\n",
    "    X_val_test = df_val_test_scaled[features_list].values\n",
    "    X_train_val = df_train_val_scaled[features_list].values\n",
    "    if target_feature == '':\n",
    "        X_full = df_full_scaled[features_list].values\n",
    "    else:\n",
    "        X_full = []\n",
    "    \n",
    "    \n",
    "    #### OUTPUTS ####\n",
    "    dict_model_inputs = {'X_train': X_train, \n",
    "                         'X_val': X_val, \n",
    "                         'X_test': X_test, \n",
    "                         'X_val_test': X_val_test, \n",
    "                         'X_train_val': X_train_val,\n",
    "                         'X_full': X_full, \n",
    "                         'y_train': y_train, \n",
    "                         'y_val': y_val, \n",
    "                         'y_test': y_test, \n",
    "                         'y_val_test': y_val_test, \n",
    "                         'y_train_val': y_train_val\n",
    "                        }\n",
    "    \n",
    "    return dict_model_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
