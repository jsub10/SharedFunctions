{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include-Utility-Functions\n",
    "\n",
    "Utilty functions that are useful in many situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a list is empty -- including the case when it contains other empty lists\n",
    "## https://stackoverflow.com/questions/1593564/python-how-to-check-if-a-nested-list-is-essentially-empty\n",
    "def isListEmpty(inList):\n",
    "    if isinstance(inList, list): # Is a list\n",
    "        return all( map(isListEmpty, inList) )\n",
    "    return False # Not a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten a list of lists\n",
    "## https://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python\n",
    "def flatten_list(list_to_flatten):\n",
    "    return [item for sublist in list_to_flatten for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from a list of lists\n",
    "## From https://stackoverflow.com/questions/2213923/removing-duplicates-from-a-list-of-lists\n",
    "## see Paul Stephenson's solution lower down on the page\n",
    "## Note: Does NOT require import itertools\n",
    "def remove_duplicate_lists(list_of_lists):\n",
    "    t0 = time.time()\n",
    "    k = list_of_lists\n",
    "    new_list_of_lists = []\n",
    "    for elem in list_of_lists:\n",
    "        if elem not in new_list_of_lists:\n",
    "            new_list_of_lists.append(elem)\n",
    "    t1 = time.time()\n",
    "    print(\"Input list has {} items.\".format(len(list_of_lists)))\n",
    "    print(\"Number of duplicates found = {}.\".format(len(list_of_lists) - len(new_list_of_lists)))\n",
    "    print(\"Duplicates removed in {} seconds.\".format(round(t1-t0,3)))\n",
    "    print(\"Deduplicated list has {} items.\".format(len(new_list_of_lists)))\n",
    "    \n",
    "    return new_list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(alist, wanted_parts=1):\n",
    "    '''\n",
    "    From https://stackoverflow.com/questions/752308/split-list-into-smaller-lists-split-in-half\n",
    "    Split a list into any number of wanted_parts\n",
    "    '''\n",
    "    length = len(alist)\n",
    "    return [ alist[i*length // wanted_parts: (i+1)*length // wanted_parts] \n",
    "             for i in range(wanted_parts) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list, get all n-tuples of it both ordered and unordered\n",
    "## https://stackoverflow.com/questions/6499327/the-pythonic-way-to-generate-pairs\n",
    "def get_n_tuples(input_list, n_size=2, ordered=0): \n",
    "    if ordered == 0:\n",
    "        list_tuples = itertools.combinations(input_list, n_size)\n",
    "    else:\n",
    "        # NOTE: no matter n_size, this will always produce pairs, i.e., n_size=2\n",
    "        list_tuples = itertools.product(input_list, input_list)\n",
    "        \n",
    "    return list(list_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of lists, get all n-tuples from the lists\n",
    "## n of the tuple will be the number of lists in the list of lists\n",
    "## https://stackoverflow.com/questions/798854/all-combinations-of-a-list-of-lists\n",
    "def get_all_tuples(input_list_of_lists, ordered=0): \n",
    "    if ordered == 0:\n",
    "        list_tuples = set(itertools.product(*input_list_of_lists))\n",
    "    else:\n",
    "        list_tuples = itertools.product(*input_list_of_lists)\n",
    "        \n",
    "    return list(list_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_paths_through_lists(list_of_lists): \n",
    "    '''\n",
    "    # From https://stackoverflow.com/questions/16108736/recursive-all-paths-through-a-list-of-lists-python\n",
    "    l = [['asdf', 'b'], 'c', ['d', 'e'], ['f', 'g'], 'h']\n",
    "    for i in itertools.product(*l): print(list(i)) returns\n",
    "    \n",
    "    ['asdf', 'c', 'd', 'f', 'h']\n",
    "    ['asdf', 'c', 'd', 'g', 'h']\n",
    "    ['asdf', 'c', 'e', 'f', 'h']\n",
    "    ['asdf', 'c', 'e', 'g', 'h']\n",
    "    ['b', 'c', 'd', 'f', 'h']\n",
    "    ['b', 'c', 'd', 'g', 'h']\n",
    "    ['b', 'c', 'e', 'f', 'h']\n",
    "    ['b', 'c', 'e', 'g', 'h']\n",
    "    \n",
    "    NOTE: the list of lists must be like l above -- \n",
    "    '''\n",
    "    # All possible paths through the elements of the list_of_lists\n",
    "    all_paths = []\n",
    "    for i in itertools.product(*list_of_lists): \n",
    "        all_paths.append(list(i))\n",
    "        \n",
    "    return all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the unique elements of a list WITHOUT altering the original order in which \n",
    "## the elements of the list appear\n",
    "def get_unique_elements_same_order(list_of_elements): \n",
    "    a = np.array(list_of_elements)\n",
    "    _, idx = np.unique(a, return_index=True)\n",
    "\n",
    "    return a[np.sort(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times a list of items appears in a list of lists\n",
    "def count_list_freq(list_of_lists):\n",
    "    '''\n",
    "    Given a list of lists, e.g., [['A', 'B', 'C'], ['D'], ['D'], ['A', 'D'], ...['A', 'B', 'C']], \n",
    "    count the frequency of each list item in the list of lists.\n",
    "    \n",
    "    Does NOT require Counter or any of the itertools\n",
    "    \n",
    "    USES remove_duplicate_lists defined above\n",
    "    '''\n",
    "    # Deduplicate the list of lists\n",
    "    deduped_list_of_lists = remove_duplicate_lists(list_of_lists)\n",
    "    \n",
    "    # For each list in the deduplicated list of lists, compare it to every list in list of lists\n",
    "    counts = []\n",
    "    for el in deduped_list_of_lists:\n",
    "        truth_list = [el == list_of_lists[n] for n in range(len(list_of_lists))]\n",
    "        # Count the number of Trues in the truth_list\n",
    "        count = len([x for x in truth_list if x == True])\n",
    "        counts.append(count)\n",
    "        \n",
    "    return np.transpose([deduped_list_of_lists, counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of times the elements of ref_list occur in list_to_check\n",
    "## See https://stackoverflow.com/questions/55809846/count-occurrences-of-list-items-in-second-list-in-python\n",
    "\n",
    "def get_sum_list_occurrences(list_to_check, ref_list): \n",
    "    # For example, list_to_check = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 6] and \n",
    "    ## ref_list = [1, 3, 6, 9]\n",
    "    ## The function returns the total number of times the elements in ref_list appear in list_to_check\n",
    "    \n",
    "    return sum(i in ref_list for i in list_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://folk.idi.ntnu.no/mlh/hetland_org/coding/python/levenshtein.py\n",
    "\n",
    "# This is a straightforward implementation of a well-known algorithm, and thus\n",
    "# probably shouldn't be covered by copyright to begin with. But in case it is,\n",
    "# the author (Magnus Lie Hetland) has, to the extent possible under law,\n",
    "# dedicated all copyright and related and neighboring rights to this software\n",
    "# to the public domain worldwide, by distributing it under the CC0 license,\n",
    "# version 1.0. This software is distributed without any warranty. For more\n",
    "# information, see <http://creativecommons.org/publicdomain/zero/1.0>\n",
    "\n",
    "def levenshtein(a,b):\n",
    "    \"Calculates the Levenshtein distance between a and b.\"\n",
    "    n, m = len(a), len(b)\n",
    "    if n > m:\n",
    "        # Make sure n <= m, to use O(min(n,m)) space\n",
    "        a,b = b,a\n",
    "        n,m = m,n\n",
    "        \n",
    "    current = range(n+1)\n",
    "    for i in range(1,m+1):\n",
    "        previous, current = current, [i]+[0]*n\n",
    "        for j in range(1,n+1):\n",
    "            add, delete = previous[j]+1, current[j-1]+1\n",
    "            change = previous[j-1]\n",
    "            if a[j-1] != b[i-1]:\n",
    "                change = change + 1\n",
    "            current[j] = min(add, delete, change)\n",
    "            \n",
    "    return current[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex\n",
    "\n",
    "A helpful guide is: https://developers.google.com/edu/python/regular-expressions\n",
    "<code>\n",
    "str = 'an example word:cat!!'\n",
    "match = re.search(r'word:\\w\\w\\w', str)\n",
    "#If-statement after search() tests if it succeeded\n",
    "if match:\n",
    "  print('found', match.group()) ## 'found word:cat'\n",
    "else:\n",
    "  print('did not find')\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render a dataframe as an image for easy cutting and pasting into documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FROM https://stackoverflow.com/questions/19726663/how-to-save-the-pandas-dataframe-series-data-as-a-figure\n",
    "def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=12,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "    \n",
    "    # Auto-size the width of just the column with index = 1,2,3\n",
    "    #mpl_table.auto_set_column_width(col=[1,2,3])\n",
    "    # Auto-size the width of all columns\n",
    "    mpl_table.auto_set_column_width(col=list(range(len(data.columns))))\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    \n",
    "    return ax.get_figure(), ax\n",
    "\n",
    "    #fig,ax = render_mpl_table(df, header_columns=0, col_width=2.0)\n",
    "    #fig.savefig(\"table_mpl.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV Files\n",
    "#pandas.read_csv(filepath_or_buffer: Union[str, pathlib.Path, IO[~AnyStr]], \n",
    "#                sep=',', delimiter=None, header='infer', names=None, \n",
    "#                index_col=None, usecols=None, squeeze=False, prefix=None, \n",
    "#                mangle_dupe_cols=True, dtype=None, engine=None, converters=None, \n",
    "#                true_values=None, false_values=None, skipinitialspace=False, \n",
    "#                skiprows=None, skipfooter=0, nrows=None, na_values=None, \n",
    "#                keep_default_na=True, na_filter=True, verbose=False, \n",
    "#                skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, \n",
    "#                keep_date_col=False, date_parser=None, dayfirst=False, \n",
    "#                cache_dates=True, iterator=False, chunksize=None, \n",
    "#                compression='infer', thousands=None, decimal=b'.', \n",
    "#                lineterminator=None, quotechar='\"', quoting=0, doublequote=True, \n",
    "#                escapechar=None, comment=None, encoding=None, dialect=None, \n",
    "#                error_bad_lines=True, warn_bad_lines=True, \n",
    "#                delim_whitespace=False, low_memory=True, \n",
    "#                memory_map=False, float_precision=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Excel Files\n",
    "#pandas.read_excel(file_name, sheet_name=0, header=0, names=None, \n",
    "#                  index_col=None, usecols=None, squeeze=False, \n",
    "#                  dtype=None, engine=None, converters=None, \n",
    "#                  true_values=None, false_values=None, skiprows=None, \n",
    "#                  nrows=None, na_values=None, keep_default_na=True, \n",
    "#                  verbose=False, parse_dates=False, date_parser=None, \n",
    "#                  thousands=None, comment=None, skip_footer=0, skipfooter=0, \n",
    "#                  convert_float=True, mangle_dupe_cols=True, **kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names and sizes of files in a directory\n",
    "#### NOTE: dir_path MUST be a directory containing a bunch of files ####\n",
    "def get_file_info(dir_path):\n",
    "    file_names = []\n",
    "    file_sizes = []\n",
    "    files = os.listdir(dir_path)\n",
    "    for name in files:\n",
    "        file_names.append(name)\n",
    "        size = os.path.getsize(os.path.join(dir_path, name))\n",
    "        file_sizes.append(size)\n",
    "        print(\"File: {}  Size: {:.2f} KB\".format(name, size/1000.))\n",
    "    \n",
    "    print(\"Total number of files loaded = {}\".format(len(file_names)))\n",
    "    \n",
    "    return [file_names, file_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the contents of a Word file using the Python docx2txt package\n",
    "# ## From https://stackoverflow.com/questions/36001482/read-doc-file-with-python\n",
    "# import docx2txt\n",
    "# def getWordDocText(file_path):\n",
    "#     file_text = docx2txt.process(file_path)\n",
    "#     #print(file_text)\n",
    "#     return file_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of each attribute and the first n values for that attribute in the data set\n",
    "#### SET n HERE ####\n",
    "display_n = 3\n",
    "\n",
    "def get_first_n_vals(dataFrame, n=display_n):\n",
    "    feature_list = list(dataFrame)\n",
    "    first_n = [list(dataFrame[attribute][0:n]) for attribute in feature_list]\n",
    "    return list(enumerate(list(zip(feature_list, first_n))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the column names of a dataframe that have 1 or more NaN values\n",
    "## Sort the column names in alphabetical order\n",
    "def get_nan_column_names(dataFrame):\n",
    "    nan_cols = list(dataFrame[dataFrame.columns[dataFrame.isna().any()]])\n",
    "    return sorted(nan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a dataframe, get all [row,column] indices that contain null/NaN values\n",
    "## From maxymoo at https://stackoverflow.com/questions/33641231/retrieve-indices-of-nan-values-in-a-pandas-dataframe\n",
    "import scipy.sparse as sp\n",
    "def get_nan_indices(dataFrame):\n",
    "    x,y = sp.coo_matrix(dataFrame.isnull()).nonzero()\n",
    "    return list(zip(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each feature, how many/what percentage of rows are missing values?\n",
    "# From https://datascience.stackexchange.com/questions/12645/\n",
    "\n",
    "def num_missing_values_per_feature(dataFrame, display='percentage'):\n",
    "    if display == 'count':\n",
    "        return dataFrame.isnull().sum(axis=0)\n",
    "    else:\n",
    "        return dataFrame.isnull().sum(axis=0)/len(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row, how many/what percentage of features in the row are missing values?\n",
    "## Output will be a list containing a value for each row.\n",
    "# From https://datascience.stackexchange.com/questions/12645/\n",
    "\n",
    "def num_missing_values_per_row(dataFrame, display='percentage'):\n",
    "    if display == 'count':\n",
    "        return dataFrame.isnull().sum(axis=1)\n",
    "    else:\n",
    "        return dataFrame.isnull().sum(axis=1)/len(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and replace all empty strings of any length in a dataframe with given value\n",
    "## For example, new_value is 'EMPTY'\n",
    "def replace_empty_strings(dataFrame, new_value='EMPTYCELL'): \n",
    "    df = dataFrame.replace(r'^\\s*$', new_value, regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a dataframe, get all [row, column] indices that contain empty strings of any length\n",
    "import scipy.sparse as sp\n",
    "def get_empty_indices(dataFrame, new_value='EMPTYCELL'):\n",
    "    # Replace empty strings in dataframe with a standard default string\n",
    "    df = replace_empty_strings(dataFrame, new_value=new_value)\n",
    "    # Now get the indices as a boolean dataframe\n",
    "    df_2 = df.isin([new_value])\n",
    "    # Get the indices as a list\n",
    "    x,y = sp.coo_matrix(df_2).nonzero()\n",
    "    return list(zip(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a dataframe field into a datetime field\n",
    "def convert_to_datetime(dataFrame, field_name):\n",
    "    dataFrame[field_name] = pd.to_datetime(dataFrame[field_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric(dataFrame, field_name, field_type=int):\n",
    "    '''\n",
    "    Convert a string-typed field_name (typically read in as type \"Object\") in a dataFrame\n",
    "    to a numeric field_type. Also, convert the NaNs to zero.\n",
    "    Return a list of the converted field values. \n",
    "    \n",
    "    field_type can be int or float\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    field_values = []\n",
    "    for value in dataFrame[field_name].values:\n",
    "        if type(value) == str:\n",
    "            # Remove the \",\" separator and convert to a number\n",
    "            field_values.append(field_type(value.replace(',', '')))\n",
    "        elif np.isnan(value):\n",
    "            # Convert to zero value\n",
    "            field_values.append(field_type('0'))\n",
    "        else:\n",
    "            field_values.append(field_type(value))\n",
    "            \n",
    "    return field_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group a dataframe that contains a datetime column by months or years\n",
    "def get_counts_by_date(dataFrame, datetime_field_name, period='month'):\n",
    "    \n",
    "    # period = 'month' (default) or 'year'\n",
    "    if period == 'month':\n",
    "        freq = '1M'\n",
    "    else:\n",
    "        freq = '1Y'\n",
    "    \n",
    "    count = dataFrame.groupby(pd.Grouper(key=datetime_field_name, freq=freq)).count()\n",
    "    if period == 'month':\n",
    "        count.index = count.index.strftime('%Y %B')\n",
    "    else:\n",
    "        count.index = count.index.strftime('%Y')\n",
    "    \n",
    "    # list(dataFrame[0]) picks a column name that is different from the datetime_field_name\n",
    "    ## just how the syntax works to build the index and values\n",
    "    names = count[list(dataFrame)[0]].index\n",
    "    counts = count[list(dataFrame)[0]].values\n",
    "    \n",
    "    return names, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize (save and load) a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle a list \n",
    "def pickle_list(list_to_pickle, destination):\n",
    "    with open(destination, 'wb') as f:\n",
    "        pickle.dump(list_to_pickle, f)\n",
    "        \n",
    "    print(\"List has been pickled to {}\".format(destination))\n",
    "    print(\"Unpickle the list using load_list(destination)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pickled list\n",
    "def load_list(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        unpickled_list = pickle.load(f)\n",
    "    \n",
    "    print(\"Item has been loaded from to {}\".format(file_path))\n",
    "    return unpickled_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle a dataframe\n",
    "def pickle_df(dataFrame, destination):\n",
    "    pd.to_pickle(dataFrame, destination)\n",
    "    print(\"Dataframe has been pickled to {}\".format(destination))\n",
    "    print(\"Unpickle the dataframe using load_df(destination)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but named consistently\n",
    "def load_df(destination):\n",
    "    print(\"Dataframe {} is unpickled and ready for use.\".format(destination))\n",
    "    print(\"Pickle the dataframe using pickle_df(dataFrame, destination)\")\n",
    "    return pd.read_pickle(destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEPRECATED ##\n",
    "## USE load_df INSTEAD ##\n",
    "def unpickle_df(destination):\n",
    "    print(\"Dataframe {} is unpickled and ready for use.\".format(destination))\n",
    "    print(\"Pickle the dataframe using pickle_df(dataFrame, destination)\")\n",
    "    return pd.read_pickle(destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another version of unpickle_df\n",
    "def read_from_pickle(path):\n",
    "    print(\"Trying...\")\n",
    "    with open(path, 'rb') as file:\n",
    "        try:\n",
    "            while True:\n",
    "                yield pickle.load(file)\n",
    "        except EOFError:\n",
    "            print(\"Arrived at the end of the file.\")\n",
    "            pass\n",
    "    \n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and retrieve a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a numpy array\n",
    "## DEPRECATED ##\n",
    "## USE pickle_array INSTEAD ##\n",
    "def save_array(array_to_save, destination):\n",
    "    np.save(destination, array_to_save)\n",
    "    print(\"The numpy array has been saved to {}\".format(destination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a numpy array -- same as above but named consistently\n",
    "## NOTE: \".npy\" is added as an extention to file_path_and_name\n",
    "def pickle_array(array_to_save, file_path_and_name):\n",
    "    print(f'NOTE: The extension .npy is automatically added to the file name {file_path_and_name}')\n",
    "    np.save(file_path_and_name, array_to_save)\n",
    "    print(f'The numpy array has been saved to {file_path_and_name}.npy')\n",
    "    print(f'Load the array using load_array({file_path_and_name}.npy)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved numpy array\n",
    "def load_array(file_path_and_name):\n",
    "    return np.load(file_path_and_name, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n choose k permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://www.geeksforgeeks.org/permutation-and-combination-in-python/\n",
    "## and https://stackoverflow.com/questions/40850892/\n",
    "\n",
    "from itertools import permutations\n",
    "def n_choose_k(k_list, num_to_choose, order_matters=1):\n",
    "    # k_list is the list of all possible values, e.g., [1,2,3,4,5]\n",
    "    # num_to_choose is the number of items to choose from the list\n",
    "    perm = list(permutations(k_list, num_to_choose))\n",
    "    if order_matters == 0:\n",
    "        # ordering doesn't matter -- remove all duplicate sets\n",
    "        perm = list(set(tuple(sorted(t)) for t in perm))\n",
    "        \n",
    "    return perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'), ('b', 'd'), ('a', 'c'), ('c', 'd'), ('a', 'd'), ('b', 'c')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_choose_k(['a', 'b', 'c', 'd'], 2, order_matters=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From https://stackoverflow.com/questions/3099987/generating-permutations-with-repetitions\n",
    "def n_choose_k_with_reps(k_list, num_to_choose): \n",
    "    # n_choose_k where the same item can appear in multiple slots\n",
    "    perms = list(itertools.product(k_list, repeat=num_to_choose))\n",
    "    print(\"Number of permutations = {}.\".format(len(perms)))\n",
    "    return perms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://www.geeksforgeeks.org/python-program-to-sort-a-list-of-tuples-by-second-item/\n",
    "# Python program to sort a list of \n",
    "# tuples by the nth item using sort() \n",
    "#### NOTE: Index n starts at 0 ####\n",
    "  \n",
    "# Sort the list by any index number of the tuple \n",
    "def sort_tuple(tups, index_num=0, reverse=True):  \n",
    "    # tups is a list of tuples\n",
    "    # reverse = False sorts in Ascending order; reverse = True sorts in Descending order\n",
    "    # Defaults to always sorting by the first item of each tuple\n",
    "    #if index_num + 1 > len(tup):\n",
    "    #    return print(\"Please try a lower index number\")\n",
    "    \n",
    "    # key is set to sort using second element of  \n",
    "    # sublist lambda has been used  \n",
    "    tups.sort(key = lambda x: x[index_num], reverse=reverse)  \n",
    "    \n",
    "    return tups  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge two lists of lists\n",
    "From https://www.geeksforgeeks.org/python-merge-two-list-of-lists-according-to-first-element/\n",
    "\n",
    "Input : lst1 = [[1, 'Alice'], [2, 'Bob'], [3, 'Cara']]\n",
    "        \n",
    "        lst2 = [[1, 'Delhi'], [2, 'Mumbai'], [3, 'Chennai']]\n",
    "\n",
    "Output : [[1, 'Alice', 'Delhi'], [2, 'Bob', 'Mumbai'], [3, 'Cara', 'Chennai']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified to join each pair of lists\n",
    "def merge_two_lists_of_lists(lst1, lst2): \n",
    "    return [[a + b] for (a, b) in zip(lst1, lst2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the lowest n and highest n values in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_vals_in_list(list_of_numbers, top_n): \n",
    "    '''\n",
    "    From https://stackoverflow.com/questions/22117834/how-do-i-return-a-list-of-the-3-lowest-values-in-another-list/22118188\n",
    "    '''\n",
    "    return np.partition(list_of_numbers, top_n-1)[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO DO ####\n",
    "def highest_vals_in_list(list_of_numbers, top_n): \n",
    "    '''\n",
    "    Modified From \n",
    "    https://stackoverflow.com/questions/22117834/how-do-i-return-a-list-of-the-3-lowest-values-in-another-list/22118188\n",
    "    '''\n",
    "    return np.partition(list_of_numbers, top_n-1)[0:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the index values of an element that occurs one or more times in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/questions/176918/finding-the-index-of-an-item-in-a-list\n",
    "def get_index_values(input_list, element_in_list):\n",
    "    # input_list is a list like so: [\"bob\", \"rob\", \"bill\"] or [1,2,3,4,0,1,3]\n",
    "    ## element_in_list is the element to match\n",
    "    ## The index numbers of that element in the list are returned\n",
    "    ## If the element is a number just use a number\n",
    "    ## If the element is a string, put it in quotes\n",
    "    index_values = [i for i, j in enumerate(input_list) if j == element_in_list]\n",
    "    return index_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the indices at which any element of one list occurs in another\n",
    "\n",
    "From https://stackoverflow.com/questions/29452735/find-the-indices-at-which-any-element-of-one-list-occurs-in-another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#haystack = ['a', 'b', 'c', 'V', 'd', 'e', 'X', 'f', 'V', 'g', 'h']\n",
    "#needles = ['V', 'W', 'X', 'Y', 'Z']\n",
    "#st = set(needles)\n",
    "#print([i for i, e in enumerate(haystack) if e in st])\n",
    "# returns [3, 6, 8]\n",
    "\n",
    "def get_haystack_idx(haystack_list, needles_list):\n",
    "    st = set(needles_list)\n",
    "    return [i for i, e in enumerate(haystack_list) if e in st]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull select index values from a dataframe to understand the extent to which documents/words within a cluster are similar to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster labels from a K-Means model\n",
    "## Use the original text data to choose random samples of documents in each cluster\n",
    "## Use this to compare the similarity of documents within their cluster and outside of their clusters\n",
    "## This gives us an intuitive understanding of how the quality of the K-Means model and ulitimately the quality of the \n",
    "## vectorization of the documents\n",
    "\n",
    "def show_doc_clusters(k_means_labels, \n",
    "                      dataFrame,  \n",
    "                      num_docs_to_display=4, \n",
    "                      column_names_list=['Concatenated Text to Perform Search', 'FINAL CATEGORY', 'PRIMARY KEY INITIATIVE']\n",
    "                     ):\n",
    "    # k_means_labels come from the visualization function in SharedFunction/Include-Viz-Functions\n",
    "    ## called show_clusters\n",
    "    # num_docs_to_display is the number of documents to display at random within each group/cluster\n",
    "    # dataFrame is a dataframe containing the documents\n",
    "    # columnNames are the column names of the dataframe containing the text to be displayed\n",
    "    # How many documents fell into which cluster?\n",
    "    \n",
    "    num_clusters = len(np.unique(k_means_labels))\n",
    "    \n",
    "    rand_ids_by_group = [np.random.choice(get_index_values(k_means_labels, i), 3, replace=False) for i in range(3)]\n",
    "    df_list = []\n",
    "    for i in range(len(rand_ids_by_group)):\n",
    "        df = df_pubs[column_names_list].iloc[rand_ids_by_group[i]]\n",
    "        # Add the cluster number column\n",
    "        df.insert(loc=0, column='Cluster Number', value=i)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gini Coefficient of an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_coefficient(x):\n",
    "    \"\"\"\n",
    "    Compute the Gini Coefficient for an array of values. \n",
    "    Uniform distribution -> Gini Coefficient = 0. \n",
    "    \n",
    "    See Peter Norvig's economic simulation notebook for another way to create the Gini index.\n",
    "    \n",
    "    \"\"\"\n",
    "    diffsum = 0\n",
    "    for i, xi in enumerate(x[:-1], 1):\n",
    "        diffsum += np.sum(np.abs(xi - x[i:]))\n",
    "    return diffsum / (len(x)**2 * np.mean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal cluster size for a K-Means model\n",
    "From https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    " - Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].\n",
    "\n",
    " - Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n",
    "\n",
    "From https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/\n",
    " - Distortion: The average of the squared distances from the cluster centers of the respective clusters. Typically, the Euclidean distance metric is used.\n",
    "\n",
    " - Inertia: The sum of squared distances of samples to their closest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_clusters_optimal(doc2vec_n_gram_model, max_cluster_size):\n",
    "    '''\n",
    "    Given a list of doc2vec vectors for a document corpus, find the optimal number of clusters based \n",
    "    on silhouette, distortion, and inertia scores for each cluster size.\n",
    "    '''\n",
    "    t0 = time.time()\n",
    "    # Get the vectors - the X values for the K Means model\n",
    "    X = doc2vec_n_gram_model.docvecs.vectors_docs\n",
    "    \n",
    "    num_clusters = list(range(2, max_cluster_size+1))\n",
    "    \n",
    "    sil_vals = []\n",
    "    distortion_vals = []\n",
    "    inertia_vals = []\n",
    "    \n",
    "    for num_cluster in num_clusters:\n",
    "        t1 = time.time()\n",
    "        print(\"Running the model for {} clusters...\".format(num_cluster))\n",
    "        kmeans_model = KMeans(n_clusters=num_cluster, init=\"k-means++\", max_iter=100) \n",
    "        km = kmeans_model.fit(X)\n",
    "        labels=kmeans_model.labels_.tolist()\n",
    "        centroids = kmeans_model.cluster_centers_\n",
    "        cluster_labels = kmeans_model.fit_predict(X)\n",
    "        sil_val = silhouette_score(X, cluster_labels)\n",
    "        sil_vals.append(sil_val)\n",
    "        print(\"Silhouette score for {} clusters = {}\".format(num_cluster, sil_val))\n",
    "        distortion_val = sum(np.min(cdist(X, centroids, 'euclidean'),axis=1)) / X.shape[0]\n",
    "        distortion_vals.append(distortion_val)\n",
    "        print(\"Distortion score for {} clusters = {}\".format(num_cluster, distortion_val))\n",
    "        inertia_val = km.inertia_\n",
    "        inertia_vals.append(inertia_val)\n",
    "        print(\"Inertia score for {} clusters = {}\".format(num_cluster, inertia_val))\n",
    "        \n",
    "        t2 = time.time()\n",
    "        print(\"Results for {} clusters in {} seconds\".format(num_cluster, round(t2-t1,3)))\n",
    "        \n",
    "    print(\"All calculations complete in {} seconds\".format(round(t2-t0, 3)))\n",
    "    return sil_vals, distortion_vals, inertia_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_quality(vector_list, max_cluster_size):\n",
    "    ''' A more general function for calculating the optimal cluster size using \n",
    "    silhouette, distortion, and inertia scores.\n",
    "    '''\n",
    "    t0 = time.time()\n",
    "    # vector_list is the list of vectors for clustering\n",
    "    X = vector_list\n",
    "    \n",
    "    num_clusters = list(range(2, max_cluster_size+1))\n",
    "    \n",
    "    sil_vals = []\n",
    "    distortion_vals = []\n",
    "    inertia_vals = []\n",
    "    \n",
    "    for num_cluster in num_clusters:\n",
    "        t1 = time.time()\n",
    "        print(\"Running the model for {} clusters...\".format(num_cluster))\n",
    "        kmeans_model = KMeans(n_clusters=num_cluster, init=\"k-means++\", max_iter=100) \n",
    "        km = kmeans_model.fit(X)\n",
    "        labels=kmeans_model.labels_.tolist()\n",
    "        centroids = kmeans_model.cluster_centers_\n",
    "        cluster_labels = kmeans_model.fit_predict(X)\n",
    "        sil_val = silhouette_score(X, cluster_labels)\n",
    "        sil_vals.append(sil_val)\n",
    "        print(\"Silhouette score for {} clusters = {}\".format(num_cluster, sil_val))\n",
    "        distortion_val = sum(np.min(cdist(X, centroids, 'euclidean'),axis=1)) / len(X)\n",
    "        distortion_vals.append(distortion_val)\n",
    "        print(\"Distortion score for {} clusters = {}\".format(num_cluster, distortion_val))\n",
    "        inertia_val = km.inertia_\n",
    "        inertia_vals.append(inertia_val)\n",
    "        print(\"Inertia score for {} clusters = {}\".format(num_cluster, inertia_val))\n",
    "        \n",
    "        t2 = time.time()\n",
    "        print(\"Results for {} clusters in {} seconds\".format(num_cluster, round(t2-t1,3)))\n",
    "        \n",
    "    print(\"All calculations complete in {} seconds\".format(round(t2-t0, 3)))\n",
    "    return sil_vals, distortion_vals, inertia_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity is (among other things) a metric for measuring how well a model matches an observation to an \n",
    "# existing observation.\n",
    "## The essence of Jaccard Similarity is to find, for any two sets, the number of elements in the intersection divided by the number of elements \n",
    "## in the union of the sets.\n",
    "# Slightly modified from \n",
    "#    http://dataconomy.com/2015/04/implementing-the-five-most-popular-similarity-measures-in-python/\n",
    "def jaccard_similarity(x,y):\n",
    "    # x and y are tokenized sentences - in general, they are lists\n",
    "    #print(set(x))\n",
    "    #print(set(y))\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    \n",
    "    try:\n",
    "        jac_score = intersection_cardinality/float(union_cardinality)\n",
    "    except ZeroDivisionError:\n",
    "        jac_score = 0.\n",
    " \n",
    "    return round(jac_score, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extend the jaccard_similarity function above to any number of inputs\n",
    "## which are specified as a list of lists.\n",
    "\n",
    "# Jaccard similarity is (among other things) a metric for measuring how well a model matches an observation to an \n",
    "# existing observation. This extends jaccard_similarity above to any number of inputs.\n",
    "## The essence of Jaccard Similarity is to find, for any two sets, the number of elements in the intersection divided by the number of elements \n",
    "## in the union of the sets.\n",
    "## From https://stackoverflow.com/questions/2541752/best-way-to-find-the-intersection-of-multiple-sets\n",
    "def jaccard_similarity_general(list_of_lists):\n",
    "    # Convert the lists into sets\n",
    "    set_list = [set(item) for item in list_of_lists]\n",
    "    intersection_cardinality = len(set.intersection(*set_list))\n",
    "    union_cardinality = len(set.union(*set_list))\n",
    "    \n",
    "    try:\n",
    "        jac_score = intersection_cardinality/float(union_cardinality)\n",
    "    except ZeroDivisionError:\n",
    "        jac_score = 0.\n",
    " \n",
    "    return round(jac_score, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_sum_pos(n, total):\n",
    "    \"\"\"\n",
    "    Return a randomly chosen list of n positive integers (not including 0) summing to total.\n",
    "    Each such list is equally likely to occur.\n",
    "    \n",
    "    From https://stackoverflow.com/questions/3589214/generate-random-numbers-summing-to-a-predefined-value\n",
    "    \n",
    "    n is the number of integers in the sequence. \n",
    "    total is the total the integers need to sum up to. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    dividers = sorted(random.sample(range(1, total), n - 1))\n",
    "    return [a - b for a, b in zip(dividers + [total], [0] + dividers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_sum_nonneg(n, total):\n",
    "    \"\"\"\n",
    "    Return a randomly chosen list of n nonnegative integers (0 or greater) summing to total.\n",
    "    Each such list is equally likely to occur.\n",
    "    \n",
    "    From https://stackoverflow.com/questions/3589214/generate-random-numbers-summing-to-a-predefined-value\n",
    "    \n",
    "    n is the number of integers in the sequence. \n",
    "    total is the total the integers need to sum up to.\n",
    "    \n",
    "    USES constrained_sum_pos\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return [x - 1 for x in constrained_sum_sample_pos(n, total + n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lagrange_generating_function(Lx, Ly):\n",
    "    '''\n",
    "    Lx is a list of x values, e.g., [1,2,3,4]\n",
    "    Ly is a list of y values, e.g., [1,4,9,16]\n",
    "    \n",
    "    Lagrange_generating_function (Lx,Ly) returns the function that maps Lx to Ly. This is the generating\n",
    "    function for any series of integers (or any series of Reals).\n",
    "    \n",
    "    Description from https://en.wikipedia.org/wiki/Lagrange_polynomial\n",
    "    In numerical analysis, the Lagrange interpolating polynomial is the unique polynomial of lowest degree that interpolates a given set of data.\n",
    "    Given a data set of coordinate pairs {\\displaystyle (x_{j},y_{j})}{\\displaystyle (x_{j},y_{j})} with \n",
    "    {\\displaystyle 0\\leq j\\leq k,}{\\displaystyle 0\\leq j\\leq k,} the {\\displaystyle x_{j}}x_{j} \n",
    "    are called nodes and the {\\displaystyle y_{j}}y_{j} are called values. \n",
    "    The Lagrange polynomial {\\displaystyle L(x)}L(x) has degree {\\textstyle \\leq k}{\\textstyle \\leq k} and \n",
    "    assumes each value at the corresponding node, {\\displaystyle L(x_{j})=y_{j}.}{\\displaystyle L(x_{j})=y_{j}.}\n",
    "    \n",
    "    Code below is from https://stackoverflow.com/questions/4003794/lagrange-interpolation-in-python\n",
    "    \n",
    "    There can be more than one type of generating function for a series. \n",
    "    So there can be two distinct generating function types that generate the exact same series.\n",
    "    See https://math.stackexchange.com/questions/3201597/unique-generating-function-for-a-sequence\n",
    "    \"Of course the generating function of a sequence {an} is unique, provided that you mean the same type \n",
    "    of generating function. This is because the generating function of the sequence is a power series with \n",
    "    coefficients uniquely determined by an. For example, the ordinary generating function is just ∑anxn, \n",
    "    the exponential generating function is ∑(an/n!)xn, the Dirichlet series generating function is ∑ann−s. \n",
    "    You cannot therefore have two different generating functions (of the same type) corresponding to one sequence.\"\n",
    "    \n",
    "    '''\n",
    "    x=sympy.symbols('x')\n",
    "    if  len(Lx)!= len(Ly):\n",
    "        return 1\n",
    "    y=0\n",
    "    for k in range ( len(Lx) ):\n",
    "        t=1\n",
    "        for j in range ( len(Lx) ):\n",
    "            if j != k:\n",
    "                t=t* ( (x-Lx[j]) /(Lx[k]-Lx[j]) )\n",
    "        y+= t*Ly[k]\n",
    "    \n",
    "    # y is the function determined by the Lagrangian interpolation\n",
    "    # Simplify the function \n",
    "    y_factored = sympy.simplify(y)\n",
    "    \n",
    "    return y_factored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jensen_shannon_dist(array1, array2):\n",
    "    \n",
    "    '''\n",
    "    Find the Jensen-Shannon distance between two distributions.\n",
    "    See https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
    "    \n",
    "    array 1 is typically a distribution that looks like this: [0.17757009, 0.4953271 , 0.3271028 ]\n",
    "    \n",
    "    NOTE: array1 and array2 are one dimensional arrays -- they must be the same length.\n",
    "    \n",
    "    '''\n",
    "    return round(distance.jensenshannon(array1, array2), 2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
